{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "side_project_emotional_detector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iDxhwcDWr93-",
        "2VOQNXyDHX8g"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqpPzxMm92Rn"
      },
      "source": [
        "#[START](#start)\n",
        "\n",
        "#[data](#data)\n",
        "* [datasetting](#datasetting)\n",
        "\n",
        "#[model](#model)\n",
        "* [modelsetting](#modelsetting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ-MY1ghUh-P"
      },
      "source": [
        "get file from other drive file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDxhwcDWr93-"
      },
      "source": [
        "#測試區"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb9ZpicIq53G",
        "outputId": "7d0d0b62-8e9c-4e93-e1f8-00b067f79585"
      },
      "source": [
        "## read target video target frame\n",
        "# start = time.time()\n",
        "\n",
        "%timeit -n 10 for num in range(1,101):img = read_video_as_image(file_url, num)\n",
        "# end = time.time()\n",
        "# print(f'{(end - start):.5f}s per 100 pics by target video and frame')\n",
        "\n",
        "#read by cv2\n",
        "%timeit -n 10 for num in range(1,101):img = Image.fromarray(cv2.imread(f'/content/image/{num}.jpg'))\n",
        "\n",
        "\n",
        "#read by PIL\n",
        "%timeit -n 10  for num in range(1,101):img = Image.open(f'/content/image/{num}.jpg')\n",
        "\n",
        "#y1: 先解壓縮然後load圖片會花的時間\n",
        "#y2: 直接讀特定frame花的時間\n",
        "total_pics = 1008463\n",
        "training_epoch = 6\n",
        "x = range(total_pics * training_epoch)\n",
        "y1 = [(nums*(1/1400*180 + 1/100*0.008) if nums<total_pics else total_pics*180/1400 + nums/100*0.008) for nums in x]\n",
        "y2 = [nums/100*4 for nums in x]\n",
        "plt.plot(x, y1)\n",
        "plt.plot(x, y2) del x, y1, y2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 3.98 s per loop\n",
            "10 loops, best of 3: 348 ms per loop\n",
            "10 loops, best of 3: 7.89 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWajLJ9RtVv2",
        "outputId": "a3fa3e9f-a06d-4ca7-98d7-a134637513af"
      },
      "source": [
        "# custom is much faster than pytorch transform\n",
        "a_np = np.zeros((640,440,3))\n",
        "a_pil = Image.fromarray(np.uint8(np.zeros((640,440,3))))\n",
        "totensor = transforms.ToTensor()\n",
        "resizer = transforms.Resize(256)\n",
        "randomcroper = transforms.RandomCrop(224)\n",
        "def custom_resize(image):\n",
        "  output_size = 256\n",
        "  h, w = image.shape[:2]\n",
        "  if h > w:\n",
        "    new_h, new_w = int(h * output_size / w), output_size\n",
        "  else:\n",
        "    new_h, new_w = output_size, int(w * output_size / h)\n",
        "  return cv2.resize(image, dsize=(new_w, new_h))\n",
        "\n",
        "def custom_randomcrop(image):\n",
        "  h, w = image.shape[:2]\n",
        "  new_h, new_w = 256, 256\n",
        "  top = np.random.randint(0, h - new_h)\n",
        "  left = np.random.randint(0, w - new_w)\n",
        "\n",
        "  return image[top: top + new_h,\n",
        "                left: left + new_w]\n",
        "print('ToTensor')\n",
        "%timeit -n 10 totensor(a_pil)\n",
        "%timeit -n 10 totensor(a_np) \n",
        "%timeit -n 10 torch.from_numpy(a_np)\n",
        "print('resizer')\n",
        "%timeit -n 10 resizer(a_pil)\n",
        "%timeit -n 10 custom_resize(a_np)\n",
        "print('randomCrop')\n",
        "%timeit -n 10 randomcroper(a_pil)\n",
        "%timeit -n 10 custom_randomcrop(a_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ToTensor\n",
            "10 loops, best of 5: 2.55 ms per loop\n",
            "10 loops, best of 5: 1.94 ms per loop\n",
            "10 loops, best of 5: 1.75 µs per loop\n",
            "resizer\n",
            "10 loops, best of 5: 3.21 ms per loop\n",
            "10 loops, best of 5: 1.55 ms per loop\n",
            "randomCrop\n",
            "10 loops, best of 5: 33.1 µs per loop\n",
            "10 loops, best of 5: 7.52 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-nKzJt9uJX"
      },
      "source": [
        "#START\n",
        "<a name='start'>Pipeline Start From Here</a>\n",
        "<p><a name='data'><font size=4>1. dataing</font></a></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXYgqZZp6D7d",
        "collapsed": true,
        "cellView": "form",
        "outputId": "039dab1e-6d6c-4cfa-d074-c2b9f41154ae"
      },
      "source": [
        "#@title\n",
        "%cd .. \n",
        "!du -shc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "du: cannot read directory './proc/115/task/115/net': Invalid argument\n",
            "du: cannot read directory './proc/115/net': Invalid argument\n",
            "du: cannot read directory './proc/173/task/173/net': Invalid argument\n",
            "du: cannot read directory './proc/173/net': Invalid argument\n",
            "du: cannot read directory './proc/225/task/225/net': Invalid argument\n",
            "du: cannot read directory './proc/225/net': Invalid argument\n",
            "du: cannot read directory './proc/639/task/639/net': Invalid argument\n",
            "du: cannot read directory './proc/639/net': Invalid argument\n",
            "du: cannot read directory './proc/641/task/641/net': Invalid argument\n",
            "du: cannot read directory './proc/641/net': Invalid argument\n",
            "du: cannot read directory './proc/643/task/643/net': Invalid argument\n",
            "du: cannot read directory './proc/643/net': Invalid argument\n",
            "du: cannot access './proc/786/task/786/fd/4': No such file or directory\n",
            "du: cannot access './proc/786/task/786/fdinfo/4': No such file or directory\n",
            "du: cannot access './proc/786/fd/3': No such file or directory\n",
            "du: cannot access './proc/786/fdinfo/3': No such file or directory\n",
            "du: cannot access './content/drive': Transport endpoint is not connected\n",
            "64G\t.\n",
            "64G\ttotal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIxOFN4ObIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b9e432ee-f2b9-445b-f9f1-5498d8b0fc0c"
      },
      "source": [
        "!pip install livelossplot\n",
        "\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.init()\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "import cv2  \n",
        "import math\n",
        "import re\n",
        "import skimage.transform #io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def seed_torch(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.enabled = False\n",
        "seed_torch()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting livelossplot\n",
            "  Downloading https://files.pythonhosted.org/packages/57/26/840be243088ce142d61c60273408ec09fa1de4534056a56d6e91b73f0cae/livelossplot-0.5.4-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot) (5.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.3.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (20.9)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (5.0.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (54.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->livelossplot) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh->livelossplot) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.4\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czvV_E4cUwek"
      },
      "source": [
        "\n",
        "def get_files_path(folder_url):\n",
        "    return [os.path.join(root, file) for root, _, files in os.walk(folder_url) for file in files] \n",
        "\n",
        "def get_sorted_folder_path(url):\n",
        "    urls = get_files_path(url)\n",
        "    urls.sort(key=lambda x: list(map(int, re.findall('\\d+', x))))\n",
        "    return urls\n",
        "\n",
        "def tar_file(file_url):\n",
        "    start = time.time()\n",
        "    with tarfile.open(file_url) as file:\n",
        "        file.extractall(os.path.dirname(file_url),)\n",
        "        print(f'done! time: {time.time() - start}')\n",
        "\n",
        "def get_data(file_url):\n",
        "    with open(file_url) as file:\n",
        "        output = [elem.strip() for elem in file.readlines()]\n",
        "    return output\n",
        "\n",
        "def get_data_to_numpy(file_url):\n",
        "    with open(file_url) as file:\n",
        "        output = np.fromfile(file)\n",
        "    return output\n",
        "\n",
        "def get_frame_nums(index):\n",
        "    return (len(list(os.walk(f'/content/aff_wild_annotations_bboxes_landmarks_new/landmarks/train/{index}/'))[0][2]),\n",
        "            len(list(os.walk(f'/content/aff_wild_annotations_bboxes_landmarks_new/bboxes/train/{index}/'))[0][2]))\n",
        "\n",
        "def show_landmark_and_bbox(image, landmarks, bboxes):\n",
        "    \"\"\"\n",
        "    Show image with landmarks\n",
        "    landmarks: N images * (x, y)\n",
        "    \"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "    for i in range(4):\n",
        "        if i<3:\n",
        "            plt.plot([bboxes[i][0], bboxes[i+1][0]], [bboxes[i][1], bboxes[i+1][1]])\n",
        "        else:\n",
        "            plt.plot([bboxes[i][0], bboxes[0][0]], [bboxes[i][1], bboxes[0][1]])\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    \n",
        "#images\n",
        "def read_video_as_image(file_url, frame_idx):\n",
        "    cap = cv2.VideoCapture(file_url)   # capturing the video from the given path\n",
        "    cap.set(1, frame_idx) #current frame number\n",
        "    success, frame = cap.read()\n",
        "    rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if success else np.zeros((224,224,3))\n",
        "    cap.release()\n",
        "    # return Image.fromarray(np.uint8(rgb_img)) #use custom transform\n",
        "    return np.uint8(rgb_img)\n",
        "\n",
        "def show_image(img):\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.show\n",
        "\n",
        "def ç(file_url, pic_nums):\n",
        "    cap = cv2.VideoCapture(file_url)   # capturing the video from the given path\n",
        "    frame_rate = cap.get(5)\n",
        "    success, frame = cap.read()\n",
        "    count = 0\n",
        "    while cap.isOpened() and count<pic_nums:\n",
        "        if not success:\n",
        "            break\n",
        "        rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        show_image(rgb_img)\n",
        "        cap.set(1, cap.get(1)+frame_rate) #current frame number\n",
        "        count += 1        \n",
        "    cap.release()\n",
        "\n",
        "def read_video_and_save(file_url, target_path, frame_rate=100):\n",
        "    cap = cv2.VideoCapture(file_url)   # capturing the video from the given path\n",
        "    video_index = re.search('\\d+',file_url).group()\n",
        "    print(video_index)\n",
        "    success, frame = cap.read()\n",
        "    if os.path.exists(target_path):\n",
        "        os.chdir(target_path)\n",
        "    else:\n",
        "        os.mkdir(target_path)\n",
        "    current_frame_num = cap.get(1)\n",
        "    total_frame_num = cap.get(7)\n",
        "    while cap.isOpened() and current_frame_num < total_frame_num - 50:\n",
        "        if not success:\n",
        "            break\n",
        "        rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        cap.set(1, cap.get(1)+frame_rate) #current frame number\n",
        "        cv2.imwrite(f'video{video_index}frame{current_frame_num}.jpg', rgb_img)\n",
        "    cap.release()\n",
        "    print(f'{video_index} video done!')\n",
        "\n",
        "\n",
        "def torchvision_read_video(path):\n",
        "    \"\"\"package問題\"\"\"\n",
        "    torchvision.io.read_video(path)\n",
        "\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\n",
        "    for item in os.listdir(src):\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        print(s)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks, ignore)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "def remove_folder(path):\n",
        "    shutil.rmtree(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFlOHQ2uwCLW",
        "outputId": "ee3c8db7-eb11-4218-eabc-e603935ebe23"
      },
      "source": [
        "root = '/content/aff_wild_annotations_bboxes_landmarks_new/videos/train'\n",
        "for file in os.listdir(root):\n",
        "    path = os.path.join(root, file)\n",
        "    read_video_and_save(path, '/content/pics')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2UIf1R34cK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a11a083-dd4c-4427-955a-6c6971d77d5b"
      },
      "source": [
        "def get_raw_data():\n",
        "    # only first time\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "    downloaded = drive.CreateFile({'id':\"1A6uU4XdO11o_VYSV5RCGiwCb9l4Wgk4s\"}) \n",
        "    downloaded.GetContentFile('aff_wild_videos_annotations_bboxes_landmarks.tar.gz')\n",
        "    downloaded.Upload()\n",
        "\n",
        "    tar_file('/content/aff_wild_videos_annotations_bboxes_landmarks.tar.gz')\n",
        "\n",
        "    src = '/content/aff_wild_annotations_bboxes_landmarks_new'\n",
        "    dst = '/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new'\n",
        "    shutil.move(src, dst)\n",
        "\n",
        "def data_prep(copy = True, tar = True):\n",
        "    if copy:\n",
        "        src = '/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new'\n",
        "        dst = '/content/aff_wild_annotations_bboxes_landmarks_new'    \n",
        "        try:\n",
        "            copytree(src, dst)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # 解壓縮\n",
        "    if tar:\n",
        "        tarfiles = get_files_path('/content/aff_wild_annotations_bboxes_landmarks_new')\n",
        "        for path in tarfiles:\n",
        "            tar_file(path)                        \n",
        "\n",
        "# get_raw_data()        \n",
        "data_prep(True, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new/annotations\n",
            "/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new/bboxes\n",
            "/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new/videos\n",
            "/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new/landmarks\n",
            "done! time: 142.20143365859985\n",
            "done! time: 26.673814058303833\n",
            "done! time: 215.93202900886536\n",
            "done! time: 45.32514190673828\n",
            "done! time: 0.17513012886047363\n",
            "done! time: 203.60403680801392\n",
            "done! time: 41.7488317489624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnDAfDM2LRAb"
      },
      "source": [
        "Try multiprocess/multithread"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "20PPxC49U6wr",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# # multi...\n",
        "# os.mkdir('aff_wild_annotations_bboxes_landmarks_new')\n",
        "# from multiprocessing import Pool, cpu_count\n",
        "# import threading\n",
        "\n",
        "# def copy_file(src_dst):\n",
        "#     print('start')\n",
        "#     print(src_dst)\n",
        "#     copytree(src_dst[0], src_dst[1])\n",
        "\n",
        "# dst = '/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new'\n",
        "# src = '/content/aff_wild_annotations_bboxes_landmarks_new'\n",
        "# srcs = [os.path.join(src, folder) for folder in os.listdir(src)]\n",
        "# dsts = [os.path.join(dst, folder) for folder in os.listdir(src)]\n",
        "# srcs_dsts = list(zip(srcs, dsts))\n",
        "\n",
        "# def multiprocess_copy_file(srcs_dsts):\n",
        "#     start = time.time()\n",
        "#     with Pool(cpu_count()) as p:\n",
        "#         p.map(copy_file, srcs_dsts)\n",
        "#     print(time.time() - start)\n",
        "#     #179.6968069076538\n",
        "\n",
        "# def multithread_copy_file(srcs_dsts):\n",
        "#     start = time.time()\n",
        "#     threads = []\n",
        "#     for i, src_dst in enumerate(srcs_dsts):\n",
        "#         threads.append(threading.Thread(target = copy_file, args = ( src_dst,)))\n",
        "#         threads[i].start()\n",
        "\n",
        "#     for i in range(4):\n",
        "#         threads[i].join()\n",
        "#     print(time.time() - start)\n",
        "\n",
        "# def multiprocess_tar_file(paths):\n",
        "#     start = time.time()\n",
        "#     with Pool(cpu_count()) as p:\n",
        "#         p.map(tar_file, paths)\n",
        "#     print(time.time() - start)\n",
        "#     #179.6968069076538\n",
        "\n",
        "# tarfiles = get_files_path('/content/aff_wild_annotations_bboxes_landmarks_new')\n",
        "# multiprocess_tar_file(tarfiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln8SdGBSHsN_"
      },
      "source": [
        "#DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qZnbZQ-_gdH"
      },
      "source": [
        "# custom data loader\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, phase='train', frame_rate=100, require_landmark=True):\n",
        "        \"\"\"\n",
        "        input format\n",
        "        dtype: numpy.float\n",
        "        [imgs, annotations(v, a), bboxes, landmarks]\n",
        "        \"\"\"   \n",
        "        assert root_dir == '/content/aff_wild_annotations_bboxes_landmarks_new'\n",
        "        \n",
        "        self.root_dir = root_dir\n",
        "        self.frame_rate = frame_rate if phase == 'train' else frame_rate*2 \n",
        "        self.transform = transform\n",
        "        self._phase = phase\n",
        "        self._require_landmark = require_landmark\n",
        "        \n",
        "        sorted_path = get_sorted_folder_path(os.path.join(root_dir, 'annotations/train'))\n",
        "        if phase == 'train':\n",
        "            sorted_path = sorted_path[:370] \n",
        "        elif phase == 'small':\n",
        "            sorted_path = sorted_path[12:14]\n",
        "        else: \n",
        "            sorted_path = sorted_path[370:]\n",
        "\n",
        "        annotations_a = [get_data(url) for url in sorted_path[::2]]\n",
        "        annotations_v = [get_data(url) for url in sorted_path[1::2]]\n",
        "        # self.annotations = [[image_index, frame_index, valence, arouse], [...]]\n",
        "\n",
        "        annotations = np.array([[re.search('\\d+', sorted_path[::2][num]).group(), frmidx, annotations_v[num][frmidx], annotations_a[num][frmidx]] \n",
        "                                for num, img in enumerate(annotations_a) \n",
        "                                for frmidx in range(50, len(img)-50)], dtype=np.float16) #-50代表後50個frame不取\n",
        "        \n",
        "        annotations = annotations[(annotations[:,2:] != 0).any(axis = 1)] # 篩選掉 valence and arousal == 0\n",
        "        self.annotations = annotations[::self.frame_rate] #以self.frame_rate 取樣\n",
        "        self._len = self.annotations.shape[0]\n",
        "\n",
        "        del sorted_path, annotations_a, annotations_v, annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert isinstance(idx, (int, list)), '必須是int or list'\n",
        "\n",
        "        image_index = int(self.annotations[idx][0])\n",
        "        frame_index = int(self.annotations[idx][1])\n",
        "        sub_file_name = '.avi' if image_index <= 200 else '.mp4'\n",
        "        \n",
        "        image_url = os.path.join(self.root_dir, f'videos/train/{image_index}{sub_file_name}')\n",
        "        bboxes_url = os.path.join(self.root_dir, f'bboxes/train/{image_index}/{frame_index}.pts')\n",
        "        landmarks_url = os.path.join(self.root_dir, f'landmarks/train/{image_index}/{frame_index}.pts')\n",
        "        \n",
        "        image = read_video_as_image(image_url, frame_index)\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'annotations': self.annotations[idx][2:],\n",
        "            'bboxes': self.get_bboxes_or_landmarks_data(bboxes_url) if os.path.exists(bboxes_url) else np.zeros((4,2), dtype=np.int8),\n",
        "            'landmarks': self.get_bboxes_or_landmarks_data(landmarks_url) if os.path.exists(landmarks_url) and self._require_landmark else np.zeros((68,2), dtype=np.int8)\n",
        "        } if image.mean() > 30 else {\n",
        "            'image': image,\n",
        "            'annotations': np.zeros((2), dtype=np.int8),\n",
        "            'bboxes': np.zeros((4, 2), dtype=np.int8),\n",
        "            'landmarks': np.zeros((68, 2), dtype=np.int8)\n",
        "        }\n",
        "        # bboxes = sample['bboxes']\n",
        "        # landmarks = sample['landmarks']\n",
        "    \n",
        "        # print(f'img: {image.mean()}, box: {bboxes.mean()}, lm: {landmarks.mean(), landmarks.shape}, annotations: {self.annotations.mean}')\n",
        "        if self.transform:      \n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "    def get_bboxes_or_landmarks_data(self, url):\n",
        "            data = [elem.split(' ') for elem in get_data(url)[3:-1]]\n",
        "            return np.array(data).astype(np.float16)\n",
        "\n",
        "\n",
        "# from torchvision import transforms, utils\n",
        "class Rescale():\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple)), '必須是 int or tuple'\n",
        "        self.output_size = output_size\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, landmarks = sample['image'], sample['bboxes'], sample['landmarks']\n",
        "        if image.mean() == 0: #讀取影片失敗會得到 np.zeros((224, 224, 3))\n",
        "            pass\n",
        "        \n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = int(h * self.output_size / w), self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, int(w * self.output_size / h)\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "        \n",
        "        image = cv2.resize(image, dsize=(new_w, new_h))\n",
        "        if (landmarks[0]!=0).any():\n",
        "            landmarks = (landmarks * [new_w/w, new_h/h]).astype(np.int16)\n",
        "        if (bboxes[0]!=0).any():\n",
        "            bboxes = (bboxes * [new_w/w, new_h/h]).astype(np.int16)\n",
        "        return {\n",
        "            'image': image,\n",
        "            'annotations': sample['annotations'],\n",
        "            'bboxes': bboxes,\n",
        "            'landmarks': landmarks\n",
        "            }\n",
        "\n",
        "class TargetRandomCrop(object):\n",
        "    \"\"\"Center Crop the image without cropping target hard.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, landmarks, annotations = sample['image'], sample['bboxes'], sample['landmarks'], sample['annotations']\n",
        "        if image.mean() == 0: #讀取影片失敗會得到 np.zeros((224, 224, 3))\n",
        "            pass\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "        max_left = w - new_w\n",
        "        max_top = h - new_h\n",
        "        \n",
        "        if (bboxes[0]!=0).any():\n",
        "            box_left, box_top = bboxes[0]\n",
        "            if box_left < max_left:\n",
        "                max_left = box_left\n",
        "            if box_top < max_top:\n",
        "                max_top = box_top\n",
        "            left = np.random.randint(max_left*0.65, max_left+1)\n",
        "            top = np.random.randint(max_top*0.65, max_top+1)\n",
        "            bboxes = bboxes - [left, top]\n",
        "\n",
        "        else:\n",
        "            top = max_top\n",
        "            left = max_left\n",
        "           \n",
        "        image = image[top: top + new_h,\n",
        "                        left: left + new_w]\n",
        "        \n",
        "        if (landmarks[0]!=0).any():\n",
        "            landmarks = landmarks - [left, top]\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'annotations': annotations,\n",
        "            'bboxes': bboxes,\n",
        "            'landmarks': landmarks\n",
        "            }\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, landmarks, annotations = sample['image'], sample['bboxes'], sample['landmarks'], sample['annotations']\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image).float(),\n",
        "                'annotations': torch.from_numpy(annotations).float(),\n",
        "                'bboxes': torch.from_numpy(bboxes).float(),\n",
        "                'landmarks': torch.from_numpy(landmarks).float()\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXAjUG-A3ru_"
      },
      "source": [
        "#Dataset Setting\n",
        "<p><a name='datasetting'><font size=4>1. datasetting</font></a></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwapFpioSTPo"
      },
      "source": [
        "class DataConfig(object):\n",
        "    root_dir = '/content/aff_wild_annotations_bboxes_landmarks_new'\n",
        "    batch_size = 64 # batch size\n",
        "    use_gpu = True # use GPU or not\n",
        "    num_workers = 2 # how many workers for loading data\n",
        "    pin_memory = True\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dconfig = DataConfig()\n",
        "\n",
        "\n",
        "def get_dataset_dataloader(phase):\n",
        "    shuffle_dict = { 'train': True, \n",
        "                    'valid': False, \n",
        "                    'small': False }\n",
        "\n",
        "    composed_transforms = transforms.Compose([\n",
        "                                          Rescale(280),\n",
        "                                          TargetRandomCrop(224),\n",
        "                                          ToTensor()\n",
        "                                        ])\n",
        "    dataset = CustomDataset(dconfig.root_dir, transform=composed_transforms, phase=phase)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size = dconfig.batch_size,\n",
        "        shuffle = shuffle_dict[phase],\n",
        "        num_workers = dconfig.num_workers,\n",
        "        pin_memory = dconfig.pin_memory\n",
        "    )\n",
        "    return dataset, dataloader\n",
        "\n",
        "train_dataset, train_dataloader = get_dataset_dataloader('train')\n",
        "valid_dataset, valid_dataloader = get_dataset_dataloader('valid')\n",
        "\n",
        "# for dataset in [small_dataset, train_dataset, valid_dataset]:\n",
        "#     print(len(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHKG7kP2j2uE",
        "outputId": "1f1e9010-f52b-42c9-cc05-5f19200f2600"
      },
      "source": [
        "for ds in [train_dataset, valid_dataset]:\n",
        "    print(len(ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7558\n",
            "1083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHEz46QX9992"
      },
      "source": [
        "#MODEL\n",
        "<a name='model'><font size=4>2. modeling</font></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCpOXw9pCAKk"
      },
      "source": [
        "1. super是必須的, 不然會出錯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7A4xyABDbmn9"
      },
      "source": [
        "def make_model(model_size, pretrained=True):\n",
        "    model_dict = {18: models.resnet18,\n",
        "                  34: models.resnet34,\n",
        "                  50: models.resnet50,\n",
        "                  101: models.resnet101,\n",
        "                  152: models.resnet152 }\n",
        "    model = model_dict[model_size](pretrained=pretrained)\n",
        "    num_fts = model.fc.in_features\n",
        "    if pretrained:\n",
        "        for param in model.parameters():\n",
        "            param._require_grads = False\n",
        "    model.fc = nn.Linear(num_fts, 2)\n",
        "    \n",
        "    # summary(model, (3,224,224))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQVST5d-umDX"
      },
      "source": [
        "#save_and_load_model\n",
        "def save_loss_as_txt(loss_dict, folder_path):\n",
        "    \"\"\"\n",
        "    loss_dict: {'train': [], 'valid': [], epoch: [], time: []}\n",
        "    \"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "    file_path = os.path.join(folder_path, mconfig.model_name + '.txt')\n",
        "    with open(file_path, 'a+') as file:\n",
        "        file.write('epoch, train loss, valid loss, time\\n')\n",
        "        for t, v, e, time in zip(loss_dict['train'], loss_dict['valid'], loss_dict['epoch'], loss_dict['time']):\n",
        "            file.write(f'{e}, {t}, {v}, {time}\\n')\n",
        "\n",
        "def save_model(PATH):\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "def load_model(PATH):\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    model.eval()\n",
        "\n",
        "def save_general_checkpoint(PATH, epoch):\n",
        "    folder_path = os.path.dirname(PATH)\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.cpu().state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,            \n",
        "            }, PATH)\n",
        "\n",
        "def load_general_checkpoint(PATH):\n",
        "    checkpoint = torch.load(PATH, map_location=dconfig.device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    model.to(dconfig.device)\n",
        "    for model_params, optim_params in optimizer.state.items():\n",
        "        for k, v in optim_params.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                optim_params[k] = v.to(dconfig.device)\n",
        "\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    # model.eval()\n",
        "    model.train()\n",
        "    return model, optimizer, epoch, loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSEHWxQ83db-"
      },
      "source": [
        "#Model Setting\n",
        "<p><a name='modelsetting'><font size=4>1. modelsetting</font></a></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjtUGAk2nYbt"
      },
      "source": [
        "class ModelConfig:\n",
        "    model_root_folder = '/content/gdrive/MyDrive/SideProject/model/'\n",
        "    loss_graph_root_folder = '/content/gdrive/MyDrive/SideProject/loss_graph/'\n",
        "    \n",
        "    model_name = 'res34_bsize64_adam'\n",
        "    resnet_size = 34\n",
        "    pretrained = True\n",
        "    model_name_suffix = 'epoch12' #only needed when load model\n",
        "    print_freq = len(train_dataset)/dconfig.batch_size/4 # print info every N batch\n",
        "    save_freq = 4\n",
        "    max_epoch = 20\n",
        "    lr = 0.001 # initial learning rate\n",
        "    lr_decay = 0.95 # when val_loss increase, lr = lr*lr_decay\n",
        "    weight_decay = 1e-4 # 损失函数\n",
        "\n",
        "    model_save_folder = os.path.join(model_root_folder, model_name)\n",
        "    target_model_path = os.path.join(model_save_folder , model_name_suffix)\n",
        "    loss_graph_folder_path = os.path.join(loss_graph_root_folder, model_name)\n",
        "\n",
        "\n",
        "mconfig = ModelConfig()   \n",
        "model = make_model(mconfig.resnet_size, pretrained=mconfig.pretrained)\n",
        "loss = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.fc.parameters())\n",
        "last_epoch = 0\n",
        "writer = SummaryWriter(log_dir=mconfig.loss_graph_folder_path, filename_suffix=mconfig.model_name)\n",
        "plotlosses = PlotLosses()\n",
        "model, optimizer, last_epoch, loss = load_general_checkpoint(mconfig.target_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ptCGWWKdpjX"
      },
      "source": [
        "def train_model(train_iter, valid_iter, model, loss, optimizer, start_epoch):\n",
        "    model.to(dconfig.device).float()\n",
        "    loss_dict = {'train': [], 'valid': [], 'epoch': [], 'time': []}\n",
        "\n",
        "    for epoch in range(mconfig.max_epoch):\n",
        "        start = time.time()\n",
        "        now_epoch = epoch + start_epoch\n",
        "\n",
        "        for phase, data_iter in [('train', train_iter), ('valid', valid_iter)]:\n",
        "            l, epoch_loss, n, batch_count = 0.0, 0.0, 0, 0\n",
        "            print(f'{phase}{now_epoch} start!')\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            for sample in data_iter:\n",
        "                X, y = sample['image'].float().to(dconfig.device), sample['annotations'].float().to(dconfig.device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    y_hat = model(X)\n",
        "                    l = loss(y_hat, y)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        l.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                epoch_loss += l.detach().cpu().item()\n",
        "                n += dconfig.batch_size\n",
        "                batch_count += 1 \n",
        "\n",
        "                if batch_count % mconfig.print_freq == 0:\n",
        "                    print(f'{batch_count} batch, time: {time.time() - start}')\n",
        "\n",
        "            epoch_loss = epoch_loss * n / batch_count\n",
        "            loss_dict[phase].append(epoch_loss)\n",
        "                \n",
        "\n",
        "        loss_dict['epoch'].append(now_epoch)\n",
        "        loss_dict['time'].append(time.time() - start)\n",
        "        writer.add_scalar(mconfig.model_name+\"_Loss/train\", loss_dict['train'][-1], now_epoch)\n",
        "        writer.add_scalar(mconfig.model_name+\"_Loss/valid\", loss_dict['valid'][-1], now_epoch)\n",
        "        writer.flush()\n",
        "\n",
        "        plotlosses.update({'loss': loss_dict['train'][-1], 'val_loss': loss_dict['valid'][-1]})        \n",
        "        plotlosses.send()\n",
        "        \n",
        "        print(f'epoch: {now_epoch}, train_loss: {loss_dict[\"train\"][-1]:.4f}, test_loss: {loss_dict[\"valid\"][-1]:.4f}, epoch_time: {time.time() - start:.1f} sec')\n",
        "        \n",
        "        if now_epoch % mconfig.save_freq == 0:\n",
        "            filename =  'epoch' + str(now_epoch)\n",
        "            PATH = os.path.join(mconfig.model_save_folder, filename)\n",
        "            save_general_checkpoint(PATH, now_epoch)\n",
        "            save_loss_as_txt(loss_dict, mconfig.model_save_folder)\n",
        "            model.to(dconfig.device)\n",
        "            loss_dict = {'train': [], 'valid': [], 'epoch': [], 'time': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CfMJbEPO1lym",
        "outputId": "824572f8-a951-41fa-fb2c-5c1b0d86f1aa"
      },
      "source": [
        "print(train_dataloader.batch_size, optimizer, loss)\n",
        "train_model(train_dataloader, valid_dataloader, model, loss, optimizer, start_epoch=last_epoch+1)\n",
        "#pretrained_resnet34_bsize64_adam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ") MSELoss()\n",
            "train13 start!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC7GWQyBJXMq"
      },
      "source": [
        "!tensorboard dev upload --logdir test \\\n",
        "--name \"My latest experiment\" \\\n",
        "--description \"Simple comparison of several hyperparameters\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uZfZMSip6C-2",
        "collapsed": true,
        "outputId": "6afa2554-54fc-4ed7-a9a6-2fb5ff1e0b66"
      },
      "source": [
        "# model = make_model(18, pretrained=True)\n",
        "#對照組\n",
        "# 1. model:\n",
        "#    1.1 pretrained=False \n",
        "#    1.2 bigmodel\n",
        "# 2. loss\n",
        "#    2.1 smoothL1\n",
        "#    2.2 \n",
        "# 3. optimizer \n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss\n",
            "\tvalidation       \t (min: 3787.080, max: 4099.358, cur: 4099.358)\n",
            "\ttraining         \t (min: 4518.642, max: 4580.432, cur: 4557.609)\n",
            "train_loss\n",
            "\ttraining         \t (min: 2209.567, max: 2209.567, cur: 2209.567)\n",
            "epoch: 6, train_loss: 4557.6095, test_loss: 4099.3580, epoch_time: 2547.3 sec\n",
            "train7 start!\n",
            "1000 batch, time: 628.7625780105591\n",
            "valid7 start!\n",
            "1000 batch, time: 1949.7109158039093\n",
            "epoch: 7, train_loss: 4458.9742, test_loss: 4099.3580, epoch_time: 2583.2 sec\n",
            "train8 start!\n",
            "1000 batch, time: 633.287703037262\n",
            "valid8 start!\n",
            "1000 batch, time: 1949.9265983104706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2ed0820534be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# littel dataset with resnet18 with ADAM(default setting)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-33c3fe892f3a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_iter, valid_iter, model, loss, optimizer, max_batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{phase}{epoch+1} start!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEOjgjbjxbuv"
      },
      "source": [
        "# grad_cam\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    grad_block.append(grad_out[0].detach())\n",
        "\n",
        "\n",
        "def farward_hook(module, input, output):\n",
        "    fmap_block.append(output)\n",
        "\n",
        "def comp_class_vec(output_vec, index=None):\n",
        "    \"\"\"\n",
        "    计算类向量\n",
        "    :param ouput_vec: tensor\n",
        "    :param index: int，指定类别\n",
        "    :return: tensor\n",
        "    \"\"\"\n",
        "    print(output_vec)\n",
        "    if not index:\n",
        "        index = np.argmax(output_vec.cpu().data.numpy())\n",
        "    else:\n",
        "        index = np.array(index)\n",
        "    print(index)\n",
        "    index = index[np.newaxis, np.newaxis]\n",
        "    print(index)\n",
        "    index = torch.from_numpy(index)\n",
        "    print(index)\n",
        "    one_hot = torch.ones(1, 2).scatter_(1, index, 1).to(dconfig.device)\n",
        "    one_hot.requires_grad = True\n",
        "    print(one_hot)\n",
        "    class_vec = torch.sum(one_hot * output)  # one_hot = 11.8605\n",
        "    print(class_vec)\n",
        "    return class_vec\n",
        "\n",
        "def gen_cam(feature_map, grads):\n",
        "    \"\"\"\n",
        "    依据梯度和特征图，生成cam\n",
        "    :param feature_map: np.array， in [C, H, W]\n",
        "    :param grads: np.array， in [C, H, W]\n",
        "    :return: np.array, [H, W]\n",
        "    \"\"\"\n",
        "    cam = np.zeros(feature_map.shape[1:], dtype=np.float32)  # cam shape (H, W)\n",
        "\n",
        "    weights = np.mean(grads, axis=(1, 2))  #\n",
        "    print(f'weights.shape: {weights.shape}, cam.shape: {cam.shape}')\n",
        "    # feature_map = np.mean(feature_map, axis=0)\n",
        "    print(f'feature_map.shape: {feature_map.shape}')\n",
        "    for i, w in enumerate(weights):\n",
        "        # print(w, cam.shape)\n",
        "        cam += w * feature_map[i, :, :]\n",
        "\n",
        "    # cam = np.maximum(cam, 0)\n",
        "    cam = cv2.resize(cam, (32, 32))\n",
        "    cam -= np.min(cam)\n",
        "    cam /= np.max(cam)\n",
        "\n",
        "    return cam\n",
        "\n",
        "def show_cam_on_image(img, mask, out_dir):\n",
        "    heatmap = cv2.applyColorMap((255*mask).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    print(f'heatmap: {heatmap.shape}, img: {img.shape}')\n",
        "    cam = cv2.addWeighted(np.float32(img),0.6,heatmap,0.4,0)\n",
        "    cam = cam / np.max(cam)\n",
        "\n",
        "    path_cam_img = os.path.join(out_dir, \"cam.jpg\")\n",
        "    path_raw_img = os.path.join(out_dir, \"raw.jpg\")\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "    cv2.imwrite(path_cam_img, np.uint8(255 * cam))\n",
        "    cv2.imwrite(path_raw_img, np.uint8(255 * img))\n",
        "\n",
        "\n",
        "# def main_grad_cam(net, img_input):\n",
        "imgs = None\n",
        "labels = None\n",
        "for sample in train_dataloader:\n",
        "    imgs = sample['image']\n",
        "    labels = sample['annotations']\n",
        "    break\n",
        "\n",
        "fmap_block = []\n",
        "grad_block = []\n",
        "\n",
        "net = model.to(dconfig.device).double()\n",
        "img_input = imgs[0].unsqueeze(dim=0).to(dconfig.device)\n",
        "print(f'img_input: {img_input.shape}')\n",
        "# 注册hook\n",
        "output_dir = '/content/gdrive/MyDrive/SideProject/grad_cam'\n",
        "\n",
        "net.layer4[1].register_forward_hook(farward_hook)\n",
        "net.layer4[1].register_backward_hook(backward_hook)\n",
        "\n",
        "# forward\n",
        "output = net(img_input)\n",
        "print(f'output: {output}')\n",
        "# idx = np.argmax(output.cpu().data.numpy())\n",
        "# print(\"predict: {}\".format(classes[idx]))\n",
        "\n",
        "# backward\n",
        "net.zero_grad()\n",
        "# _loss = comp_class_vec(output)\n",
        "# _loss = loss(output, labels[7].to(dconfig.device))\n",
        "_loss = output.sum()\n",
        "print(f'loss: {_loss}')\n",
        "_loss.backward()\n",
        "\n",
        "# 生成cam\n",
        "grads_val = grad_block[0].cpu().data.numpy().squeeze()\n",
        "fmap = fmap_block[0].cpu().data.numpy().squeeze()\n",
        "print(f'grads_val: {grads_val.shape}, fmap: {fmap.shape}')\n",
        "cam = gen_cam(fmap, grads_val)\n",
        "print(f'cam: {cam.shape}')\n",
        "# 保存cam图片\n",
        "img = img_input.numpy()[::1-,:,:]\n",
        "img_show = np.float32(cv2.resize(img, (32, 32))) / 255\n",
        "print(f'img_show: {img_show.shape}')\n",
        "show_cam_on_image(img_show, cam.astype(np.uint8), output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}