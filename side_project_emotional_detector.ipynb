{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "side_project_emotional_detector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-nKzJt9uJX"
      },
      "source": [
        "#Table of contents\n",
        "1. [Dependency](#de)\n",
        "2. [Basic Function](#bf)\n",
        "3. [Config](#cfg)\n",
        "4. [Data](#data)\n",
        "5. [Model](#model)\n",
        "6. [Deploying on Flask](#dof)\n",
        "7. [Experiment](#ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AI5uwLGacYp"
      },
      "source": [
        "# <a name=\"de\">Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWnZo2Vwy-dW"
      },
      "source": [
        "!wget https://drive.google.com/file/d/1A6uU4XdO11o_VYSV5RCGiwCb9l4Wgk4s\n",
        "!pip install livelossplot\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIxOFN4ObIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "ce7901ef-ba88-4eea-898f-5647128c3713"
      },
      "source": [
        "from livelossplot import PlotLosses\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import cv2  \n",
        "import math\n",
        "import re\n",
        "import skimage.transform #io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def seed_torch(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.enabled = False\n",
        "\n",
        "seed_torch()\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# a8356107"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting livelossplot\n",
            "  Downloading https://files.pythonhosted.org/packages/57/26/840be243088ce142d61c60273408ec09fa1de4534056a56d6e91b73f0cae/livelossplot-0.5.4-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot) (5.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.3.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (20.9)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (5.0.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (56.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (1.0.18)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->livelossplot) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.5)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-77d1574f7f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m# a8356107\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivbS9ulfajHj"
      },
      "source": [
        "# <a name=\"bf\">Basic Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czvV_E4cUwek"
      },
      "source": [
        "def get_file_paths(folder_path):\n",
        "    return [os.path.join(root, file) for root, _, files in os.walk(folder_path) for file in files] \n",
        "\n",
        "def get_sorted_file_paths(folder_path):\n",
        "    sorted_paths = get_file_paths(folder_path)\n",
        "    sorted_paths.sort(key=lambda x: list(map(int, re.findall('\\d+', x))))\n",
        "    return sorted_paths\n",
        "\n",
        "def tar_file(file_path):\n",
        "    start = time.time()\n",
        "    with tarfile.open(file_path) as file:\n",
        "        file.extractall(os.path.dirname(file_path),)\n",
        "        print(f'done! time: {time.time() - start}')\n",
        "\n",
        "def get_data(file_path, dtype=\"normal\"):\n",
        "    with open(file_path) as file:\n",
        "        if dtype == \"normal\":\n",
        "            output = [elem.strip() for elem in file.readlines()]\n",
        "        elif dtype == \"numpy\":\n",
        "            output = np.fromfile(file)\n",
        "        else:\n",
        "            raise ValueError(\"arg should be either normal or numpy\")\n",
        "    return output\n",
        "\n",
        "def get_total_frame_num_of_target_video(index):\n",
        "    return (len(list(os.walk(f'/content/aff_wild_annotations_bboxes_landmarks_new/landmarks/train/{index}/'))[0][2]),\n",
        "            len(list(os.walk(f'/content/aff_wild_annotations_bboxes_landmarks_new/bboxes/train/{index}/'))[0][2]))\n",
        "\n",
        "def get_video_frame_indices_from_target_folder(folder_path):\n",
        "    paths = get_sorted_file_paths(folder_path)\n",
        "    regex = re.compile('\\d+')\n",
        "    indices = [regex.findall(path) for path in paths]\n",
        "    return np.array(indices) # video, frame\n",
        "\n",
        "def show_landmark_and_bbox(image, landmarks, bboxes):\n",
        "    \"\"\"\n",
        "    Show image with landmarks\n",
        "    landmarks: N images * (x, y)\n",
        "    \"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "    for i in range(4):\n",
        "        if i<3:\n",
        "            plt.plot([bboxes[i][0], bboxes[i+1][0]], [bboxes[i][1], bboxes[i+1][1]])\n",
        "        else:\n",
        "            plt.plot([bboxes[i][0], bboxes[0][0]], [bboxes[i][1], bboxes[0][1]])\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    \n",
        "#images\n",
        "def read_target_frame(video_path, frame_idx):\n",
        "    cap = cv2.VideoCapture(video_path)   # capturing the video from the given path\n",
        "    cap.set(1, frame_idx) #current frame number\n",
        "    success, frame = cap.read()\n",
        "    rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if success else np.zeros((224,224,3), dtype=np.uint8)\n",
        "    cap.release()\n",
        "    return rgb_img\n",
        "\n",
        "def show_first_N_frame_of_target_video(video_path, pic_nums):\n",
        "    cap = cv2.VideoCapture(video_path)   # capturing the video from the given path\n",
        "    frame_rate = cap.get(5)\n",
        "    success, frame = cap.read()\n",
        "    count = 0\n",
        "    while cap.isOpened() and count < pic_nums:\n",
        "        if not success:\n",
        "            break\n",
        "        rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        show_image(rgb_img)\n",
        "        cap.set(1, cap.get(1)+frame_rate) #current frame number\n",
        "        count += 1        \n",
        "    cap.release()\n",
        "\n",
        "def save_video_frames_to_target_folder(video_path, target_folder_path, frame_rate=100):\n",
        "    cap = cv2.VideoCapture(video_path)   # capturing the video from the given path\n",
        "    video_index = re.search('\\d+',video_path).group()\n",
        "    print(\"Video Index: \", video_index)\n",
        "    success, frame = cap.read()\n",
        "    if os.path.exists(target_folder_path):\n",
        "        os.chdir(target_folder_path)\n",
        "    else:\n",
        "        os.mkdir(target_folder_path)\n",
        "    current_frame_num = cap.get(1)\n",
        "    total_frame_num = cap.get(7)\n",
        "    while cap.isOpened() and current_frame_num < total_frame_num - 50: # do not use final 50 frames\n",
        "        if not success:\n",
        "            break\n",
        "        rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        cap.set(1, cap.get(1)+frame_rate) #current frame number\n",
        "        cv2.imwrite(f'video{video_index}frame{current_frame_num}.jpg', rgb_img)\n",
        "    cap.release()\n",
        "    print(f'{video_index} video done!')\n",
        "\n",
        "\n",
        "# def torchvision_read_video(path):\n",
        "#     \"\"\"package問題\"\"\"\n",
        "#     torchvision.io.read_video(path)\n",
        "\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\n",
        "    for item in os.listdir(src):\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        print(s)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks, ignore)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "def remove_folder(path):\n",
        "    shutil.rmtree(path)\n",
        "\n",
        "\n",
        "# raw_data_handler\n",
        "def get_raw_data():\n",
        "    \"\"\"\n",
        "    Time Cost\n",
        "        2'30 downloaded\n",
        "        2'00 tar .tar.gz\n",
        "        10'20 tar whole file 54s faster than no mp version\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')    \n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    import multiprocessing\n",
        "\n",
        "    # downloading\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "    print('start downloading!')\n",
        "    start = time.time()\n",
        "    downloaded = drive.CreateFile({'id':\"1A6uU4XdO11o_VYSV5RCGiwCb9l4Wgk4s\"}) \n",
        "    downloaded.GetContentFile('aff_wild_videos_annotations_bboxes_landmarks.tar.gz')\n",
        "    downloaded.Upload()\n",
        "    print(f'download done, time: {time.time()-start}')\n",
        "\n",
        "    # taring\n",
        "    print('start taring!')\n",
        "    tar_file('/content/aff_wild_videos_annotations_bboxes_landmarks.tar.gz')\n",
        "\n",
        "    tarfile_paths = get_file_paths('/content/aff_wild_annotations_bboxes_landmarks_new')\n",
        "    manager = multiprocessing.Manager()\n",
        "    jobs = []\n",
        "    print(f'total tarfiles: {len(tarfile_paths)}\\nstart taring...')\n",
        "    for i, path in enumerate(tarfile_paths):\n",
        "        p = multiprocessing.Process(target=tar_file, args=(path, i))\n",
        "        jobs.append(p)\n",
        "        p.start()\n",
        "            \n",
        "    for proc in jobs:\n",
        "        proc.join()\n",
        "\n",
        "def data_prep():\n",
        "    \"\"\"\n",
        "    Time Cost\n",
        "        4'?? move\n",
        "        11'14 tar\n",
        "    \"\"\"\n",
        "    src = '/content/gdrive/MyDrive/SideProject/aff_wild_annotations_bboxes_landmarks_new'\n",
        "    dst = '/content/aff_wild_annotations_bboxes_landmarks_new'\n",
        "    try:\n",
        "        copytree(src, dst)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # 解壓縮\n",
        "    tarfile_paths = get_file_paths('/content/aff_wild_annotations_bboxes_landmarks_new')\n",
        "    for path in tarfile_paths:\n",
        "            tar_file(path)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFlOHQ2uwCLW",
        "outputId": "ee3c8db7-eb11-4218-eabc-e603935ebe23"
      },
      "source": [
        "\n",
        "get_raw_data() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHOz0V2ds1vZ"
      },
      "source": [
        "# <a name=\"cfg\">Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCHr4keVpy_X"
      },
      "source": [
        "# setting\n",
        "class dconfig:\n",
        "    root_folder = '/content/aff_wild_annotations_bboxes_landmarks_new'\n",
        "    batch_size = 64 # batch size\n",
        "    use_gpu = True # use GPU or not\n",
        "    num_workers = 2 # how many workers for loading data\n",
        "    pin_memory = True\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    frame_rate = 100\n",
        "    is_landmark_required = True\n",
        "\n",
        "class mconfig:\n",
        "    model_root_folder = '/content/gdrive/MyDrive/SideProject/model/'\n",
        "    loss_graph_root_folder = '/content/gdrive/MyDrive/SideProject/loss_graph/'\n",
        "    \n",
        "    model_name = 'res34_bsize64_adam'\n",
        "    is_continued_training = False\n",
        "    target_epoch_folder_name = 'epoch12' # needed only when loading model\n",
        "    resnet_size = 34\n",
        "    is_pretrained = True\n",
        "    target_class_num = 2 # arousal and valence\n",
        "\n",
        "    print_freq = 20 # print info every N batch\n",
        "    save_freq = 4 # save model every N epoch\n",
        "    max_epoch = 20\n",
        "    lr = 0.001 # initial learning rate\n",
        "    lr_decay = 0.95 # when val_loss increase, lr = lr * lr_decay\n",
        "    weight_decay = 1e-4 # 损失函数\n",
        "\n",
        "    model_save_folder = os.path.join(model_root_folder, model_name)\n",
        "    target_model_path = os.path.join(model_save_folder , target_epoch_folder_name)\n",
        "    loss_graph_folder_path = os.path.join(loss_graph_root_folder, model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "josw4QwQpuzS"
      },
      "source": [
        "# <a name=\"data\">Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCl2Mo5vliHV"
      },
      "source": [
        "## easy Label EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0zu7dSER0bo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "aeddb3ea-094f-4b37-9d72-d37ad9cca4a1"
      },
      "source": [
        "root_folder = \"/content/aff_wild_annotations_bboxes_landmarks_new\"\n",
        "box_folder = os.path.join(root_folder, \"bboxes/train\")\n",
        "landmark_folder = os.path.join(root_folder, \"landmarks/train\")\n",
        "\n",
        "box_indices = get_video_frame_indices_from_target_folder(box_folder)\n",
        "lm_indices = get_video_frame_indices_from_target_folder(landmark_folder)\n",
        "\n",
        "print(\n",
        "f'box and landmark folder have the same indices: \\\n",
        "{(box_indices == lm_indices).all()}\\n\\\n",
        "total frames with box and landmark: {box_indices.shape[0]}'\n",
        ")\n",
        "\n",
        "sorted_annot_paths = get_sorted_file_paths(os.path.join(root_folder, 'annotations/train'))\n",
        "annotations_a = [get_data(path) for path in sorted_annot_paths[::2]]\n",
        "annotations_v = [get_data(path) for path in sorted_annot_paths[1::2]]\n",
        "\n",
        "annotations = np.array([[re.search('\\d+', sorted_annot_paths[::2][num]).group(), frmidx, annotations_v[num][frmidx], annotations_a[num][frmidx]] \n",
        "                    for num, frames in enumerate(annotations_a) \n",
        "                    for frmidx in range(len(frames))]) \n",
        "\n",
        "df_annote = pd.DataFrame(annotations, columns=['vid_idx', 'frm_idx', 'v', 'a'])\n",
        "df_box = pd.DataFrame(box_indices, columns=['vid_idx', 'frm_idx'])\n",
        "\n",
        "df_with_boxlm = df_box.merge(df_annote, on=['vid_idx', 'frm_idx'])\n",
        "\n",
        "df_zero = df_with_boxlm[(df_with_boxlm['a'] == '0.0') & (df_with_boxlm['v'] == '0.0')]\n",
        "df_zero = df_zero.astype(float)\n",
        "\n",
        "print(\"Annotations Stastics:\")\n",
        "df_annote[['v', 'a']].describe()\n",
        "\n",
        "plt.scatter(x=df_zero['vid_idx'], y=df_zero['frm_idx'])\n",
        "plt.title('scatter plot of video index vs frame index that v, a\\'s value equals to zero')\n",
        "plt.show()\n",
        "\n",
        "df_nonzero = df_with_boxlm[(df_annote['a'] != '0.0') & (df_with_boxlm['v'] != '0.0')]\n",
        "df_nonzero = df_with_boxlm.astype(float)\n",
        "\n",
        "plt.scatter(x=df_nonzero['v'], y=df_nonzero['a'])\n",
        "plt.title('scatter of not zero value, imbalanced')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-22b11ad9f3e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/aff_wild_annotations_bboxes_landmarks_new\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbox_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bboxes/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlandmark_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"landmarks/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbox_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_frame_indices_from_target_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln8SdGBSHsN_"
      },
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qZnbZQ-_gdH"
      },
      "source": [
        "# custom data loader\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_folder=dconfig.root_folder, transform=None, phase='train', frame_rate=dconfig.frame_rate, is_landmark_required=dconfig.is_landmark_required):\n",
        "        \"\"\"\n",
        "        Keyword Args:\n",
        "            phase: str, train or valid or test\n",
        "        \"\"\"           \n",
        "        self.root_folder = root_folder\n",
        "        self.frame_rate = frame_rate if phase == 'train' else frame_rate*2 \n",
        "        self.transform = transform\n",
        "        self._phase = phase\n",
        "        self._is_landmark_required = is_landmark_required\n",
        "\n",
        "        self.annotations = self._get_annotations()\n",
        "        self._len = self.annotations.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert isinstance(idx, (int, list)), '必須是int or list'\n",
        "\n",
        "        image_index = int(self.annotations[idx][0])\n",
        "        frame_index = int(self.annotations[idx][1])\n",
        "        sub_file_name = '.avi' if image_index <= 200 else '.mp4'\n",
        "        \n",
        "        image_path = os.path.join(self.root_folder, f'videos/train/{image_index}{sub_file_name}')\n",
        "        bbox_file_path = os.path.join(self.root_folder, f'bboxes/train/{image_index}/{frame_index}.pts')\n",
        "        landmark_file_path = os.path.join(self.root_folder, f'landmarks/train/{image_index}/{frame_index}.pts')\n",
        "        \n",
        "        image = read_target_frame(image_path, frame_index)\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'annotations': self.annotations[idx][2:],\n",
        "            'bboxes': self.get_bboxes_or_landmarks(bbox_file_path) if os.path.exists(bbox_file_path) else np.zeros((4,2), dtype=np.int8),\n",
        "            'landmarks': self.get_bboxes_or_landmarks(landmark_file_path) if os.path.exists(landmark_file_path) and self._is_landmark_required else np.zeros((68,2), dtype=np.int8)\n",
        "        } if image.mean() > 30 else { # black image filtered\n",
        "            'image': image,\n",
        "            'annotations': np.zeros((2), dtype=np.int8),\n",
        "            'bboxes': np.zeros((4, 2), dtype=np.int8),\n",
        "            'landmarks': np.zeros((68, 2), dtype=np.int8)\n",
        "        }\n",
        "        if self.transform:      \n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def _get_annotations(self):\n",
        "        \"\"\"Get annotation data as the folloing form:\n",
        "        annotations: np.array, [video_index, frame_index, velance, arousal]\n",
        "        \"\"\"\n",
        "\n",
        "        sorted_annot_paths = get_sorted_file_paths(os.path.join(self.root_folder, 'annotations/train'))\n",
        "        if self.phase == 'train':\n",
        "            sorted_annot_paths = sorted_annot_paths[:370] \n",
        "        elif self.phase == 'test':\n",
        "            sorted_annot_paths = sorted_annot_paths[12:14]\n",
        "        else: \n",
        "            sorted_annot_paths = sorted_annot_paths[370:]\n",
        "\n",
        "        annotations_a = [get_data(path) for path in sorted_annot_paths[::2]]\n",
        "        annotations_v = [get_data(path) for path in sorted_annot_paths[1::2]]\n",
        "\n",
        "        annotations = np.array([[re.search('\\d+', sorted_annot_paths[::2][num]).group(), frmidx, annotations_v[num][frmidx], annotations_a[num][frmidx]] \n",
        "                                for num, img in enumerate(annotations_a) \n",
        "                                for frmidx in range(50, len(img)-50)], dtype=np.float16) #-50代表後50個frame不取\n",
        "        \n",
        "        annotations = annotations[(annotations[:,2:] != 0).any(axis = 1)] # 篩選掉 valence and arousal == 0\n",
        "        return annotations[::self.frame_rate] #以self.frame_rate 取樣\n",
        "\n",
        "    def get_bboxes_or_landmarks(self, path):\n",
        "        data = [elem.split(' ') for elem in get_data(path)[3:-1]]\n",
        "        return np.array(data).astype(np.float16)\n",
        "\n",
        "\n",
        "# from torchvision import transforms, utils\n",
        "class Rescale():\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple)), '必須是 int or tuple'\n",
        "        self.output_size = output_size\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, landmarks = sample['image'], sample['bboxes'], sample['landmarks']\n",
        "        if image.mean() == 0: #讀取影片失敗會得到 np.zeros((224, 224, 3))\n",
        "            pass\n",
        "        \n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = int(h * self.output_size / w), self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, int(w * self.output_size / h)\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "        \n",
        "        image = cv2.resize(image, dsize=(new_w, new_h))\n",
        "        if (landmarks[0]!=0).any():\n",
        "            landmarks = (landmarks * [new_w/w, new_h/h]).astype(np.int16)\n",
        "        if (bboxes[0]!=0).any():\n",
        "            bboxes = (bboxes * [new_w/w, new_h/h]).astype(np.int16)\n",
        "        return {\n",
        "            'image': image,\n",
        "            'annotations': sample['annotations'],\n",
        "            'bboxes': bboxes,\n",
        "            'landmarks': landmarks\n",
        "            }\n",
        "\n",
        "class TargetRandomCrop(object):\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, landmarks, annotations = sample['image'], sample['bboxes'], sample['landmarks'], sample['annotations']\n",
        "        if image.mean() == 0: #讀取影片失敗會得到 np.zeros((224, 224, 3))\n",
        "            pass\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "        max_left = w - new_w\n",
        "        max_top = h - new_h\n",
        "        \n",
        "        if (bboxes[0]!=0).any():\n",
        "            box_left, box_top = bboxes[0]\n",
        "            if box_left < max_left:\n",
        "                max_left = box_left\n",
        "            if box_top < max_top:\n",
        "                max_top = box_top\n",
        "            left = np.random.randint(max_left*0.65, max_left+1)\n",
        "            top = np.random.randint(max_top*0.65, max_top+1)\n",
        "            bboxes = bboxes - [left, top]\n",
        "\n",
        "        else:\n",
        "            top = max_top\n",
        "            left = max_left\n",
        "           \n",
        "        image = image[top: top + new_h,\n",
        "                        left: left + new_w]\n",
        "        \n",
        "        if (landmarks[0]!=0).any():\n",
        "            landmarks = landmarks - [left, top]\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'annotations': annotations,\n",
        "            'bboxes': bboxes,\n",
        "            'landmarks': landmarks\n",
        "            }\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, landmarks, annotations = sample['image'], sample['bboxes'], sample['landmarks'], sample['annotations']\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image).float()/255.0,\n",
        "                'annotations': torch.from_numpy(annotations).float(),\n",
        "                'bboxes': torch.from_numpy(bboxes).float(),\n",
        "                'landmarks': torch.from_numpy(landmarks).float()\n",
        "                }\n",
        "\n",
        "def get_dataset_dataloader(phase):\n",
        "    shuffle_dict = { 'train': True, \n",
        "                    'valid': False, \n",
        "                    'small': False }\n",
        "\n",
        "    composed_transforms = transforms.Compose([\n",
        "                                          Rescale(280),\n",
        "                                          TargetRandomCrop(224),\n",
        "                                          ToTensor()\n",
        "                                        ])\n",
        "    dataset = CustomDataset(transform=composed_transforms, phase=phase)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size = dconfig.batch_size,\n",
        "        shuffle = shuffle_dict[phase],\n",
        "        num_workers = dconfig.num_workers,\n",
        "        pin_memory = dconfig.pin_memory\n",
        "    )\n",
        "    return dataset, dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHKG7kP2j2uE",
        "outputId": "1f1e9010-f52b-42c9-cc05-5f19200f2600"
      },
      "source": [
        "# train_dataset, train_dataloader = get_dataset_dataloader('train')\n",
        "# valid_dataset, valid_dataloader = get_dataset_dataloader('valid')\n",
        "\n",
        "# for ds in [train_dataset, valid_dataset]:\n",
        "#     print(len(ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7558\n",
            "1083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHEz46QX9992"
      },
      "source": [
        "# <a name=\"model\">MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCpOXw9pCAKk"
      },
      "source": [
        "1. super是必須的, 不然會出錯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQVST5d-umDX"
      },
      "source": [
        "#save_and_load_model\n",
        "def save_loss_as_txt(loss_dict, folder_path):\n",
        "    \"\"\"\n",
        "    loss_dict: {'train': [], 'valid': [], epoch: [], time: []}\n",
        "    \"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "    file_path = os.path.join(folder_path, mconfig.model_name + '.txt')\n",
        "    with open(file_path, 'a+') as file:\n",
        "        file.write('epoch, train loss, valid loss, time\\n')\n",
        "        for t, v, e, time in zip(loss_dict['train'], loss_dict['valid'], loss_dict['epoch'], loss_dict['time']):\n",
        "            file.write(f'{e}, {t}, {v}, {time}\\n')\n",
        "\n",
        "def save_model(PATH):\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "def load_model(PATH):\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    model.eval()\n",
        "\n",
        "def save_general_checkpoint(PATH, epoch):\n",
        "    folder_path = os.path.dirname(PATH)\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.cpu().state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,            \n",
        "            }, PATH)\n",
        "\n",
        "def load_general_checkpoint(PATH):\n",
        "    checkpoint = torch.load(PATH, map_location=dconfig.device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    model.to(dconfig.device)\n",
        "    for model_params, optim_params in optimizer.state.items():\n",
        "        for k, v in optim_params.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                optim_params[k] = v.to(dconfig.device)\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    # model.eval()\n",
        "    model.train()\n",
        "    return model, optimizer, epoch, loss\n",
        "\n",
        "\n",
        "def make_model(mconfig.resnet_size, is_pretrained=mconfig.is_pretrained):\n",
        "    model_dict = {18: models.resnet18,\n",
        "                  34: models.resnet34,\n",
        "                  50: models.resnet50,\n",
        "                  101: models.resnet101,\n",
        "                  152: models.resnet152 }\n",
        "    model = model_dict[model_size](pretrained=is_pretrained)\n",
        "    num_fts = model.fc.in_features\n",
        "    if is_pretrained:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    model.fc = nn.Linear(num_fts, mconfig.target_class_num)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjtUGAk2nYbt"
      },
      "source": [
        "train_dataset, train_dataloader = get_dataset_dataloader('train')\n",
        "valid_dataset, valid_dataloader = get_dataset_dataloader('valid')\n",
        "model = make_model()\n",
        "loss = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=mconfig.lr)\n",
        "writer = SummaryWriter(log_dir=mconfig.loss_graph_folder_path, filename_suffix=mconfig.model_name)\n",
        "plotlosses = PlotLosses()\n",
        "start_epoch = 0\n",
        "if mconfig.is_continued_training:\n",
        "    model, optimizer, start_epoch, loss = load_general_checkpoint(mconfig.target_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ptCGWWKdpjX"
      },
      "source": [
        "def train_model(train_iter, valid_iter, model, loss, optimizer, start_epoch):\n",
        "    model.to(dconfig.device).float()\n",
        "    loss_dict = {'train': [], 'valid': [], 'epoch': [], 'time': []}\n",
        "\n",
        "    for epoch in range(mconfig.max_epoch):\n",
        "        start = time.time()\n",
        "        now_epoch = epoch + start_epoch\n",
        "\n",
        "        for phase, data_iter in [('train', train_iter), ('valid', valid_iter)]:\n",
        "            l, epoch_loss, n, batch_count = 0.0, 0.0, 0, 0\n",
        "            print(f'{phase}{now_epoch} start!')\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            for sample in data_iter:\n",
        "                X, y = sample['image'].float().to(dconfig.device), sample['annotations'].float().to(dconfig.device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    y_hat = model(X)\n",
        "                    l = loss(y_hat, y)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        l.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                epoch_loss += l.detach().cpu().item()\n",
        "                n += dconfig.batch_size\n",
        "                batch_count += 1 \n",
        "\n",
        "                if batch_count % mconfig.print_freq == 0:\n",
        "                    print(f'{batch_count} batch, time: {time.time() - start}')\n",
        "\n",
        "            epoch_loss = epoch_loss * n / batch_count\n",
        "            loss_dict[phase].append(epoch_loss)\n",
        "                \n",
        "\n",
        "        loss_dict['epoch'].append(now_epoch)\n",
        "        loss_dict['time'].append(time.time() - start)\n",
        "        writer.add_scalar(mconfig.model_name+\"_Loss/train\", loss_dict['train'][-1], now_epoch)\n",
        "        writer.add_scalar(mconfig.model_name+\"_Loss/valid\", loss_dict['valid'][-1], now_epoch)\n",
        "        writer.flush()\n",
        "\n",
        "        plotlosses.update({'loss': loss_dict['train'][-1], 'val_loss': loss_dict['valid'][-1]})        \n",
        "        plotlosses.send()\n",
        "        \n",
        "        print(f'epoch: {now_epoch}, train_loss: {loss_dict[\"train\"][-1]:.4f}, test_loss: {loss_dict[\"valid\"][-1]:.4f}, epoch_time: {time.time() - start:.1f} sec')\n",
        "        \n",
        "        if now_epoch % mconfig.save_freq == 0:\n",
        "            filename =  'epoch' + str(now_epoch)\n",
        "            PATH = os.path.join(mconfig.model_save_folder, filename)\n",
        "            save_general_checkpoint(PATH, now_epoch)\n",
        "            save_loss_as_txt(loss_dict, mconfig.model_save_folder)\n",
        "            model.to(dconfig.device)\n",
        "            loss_dict = {'train': [], 'valid': [], 'epoch': [], 'time': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CfMJbEPO1lym",
        "outputId": "824572f8-a951-41fa-fb2c-5c1b0d86f1aa"
      },
      "source": [
        "print(train_dataloader.batch_size, optimizer, loss)\n",
        "train_model(train_dataloader, valid_dataloader, model, loss, optimizer, start_epoch=start_epoch+1)\n",
        "#pretrained_resnet34_bsize64_adam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ") MSELoss()\n",
            "train13 start!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhVP1J59xiTs"
      },
      "source": [
        "# TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC7GWQyBJXMq"
      },
      "source": [
        "!tensorboard dev upload --logdir test \\\n",
        "--name \"My latest experiment\" \\\n",
        "--description \"Simple comparison of several hyperparameters\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VOQNXyDHX8g"
      },
      "source": [
        "# <a name=\"dof\">Deploying on flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_LOP3z7m7BJ"
      },
      "source": [
        "# server.py\n",
        "from flask import Flask, request, jsonify\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "import io\n",
        "from io import BufferedReader, BytesIO\n",
        "from torchvision import models, transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "def io_wrap_img_2_np_img(image_io):\n",
        "    img_bytes = image_io.read()\n",
        "    image_buffer = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "    return cv2.imdecode(image_buffer, cv2.IMREAD_COLOR)\n",
        "\n",
        "def get_prediction(image_io):\n",
        "    pil_img = io_wrap_img_2_np_img(image_io)\n",
        "    transformed = transform_pil_img(pil_img)\n",
        "\n",
        "    output = model(transformed).detach().cpu()    \n",
        "    return output[0][0].item(), output[0][1].item()\n",
        "\n",
        "def transform_np_img(np_img):\n",
        "    np_img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n",
        "    np_img = cv2.resize(np_img, (224, 224))\n",
        "    tensor = torch.tensor(np_img, dtype=torch.float)    \n",
        "    return tensor.permute(2, 0, 1).div(255.0).unsqueeze(0)\n",
        "\n",
        "def get_best_model():\n",
        "    # test version\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    num_fts = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_fts, 2)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        enc_img = request.files[\"image_io\"]\n",
        "        v, a = get_prediction(enc_img) \n",
        "    return jsonify({'velance': v, 'arousal': a})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = get_best_model()\n",
        "    app.run(debug=True, port=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8vKMU0DyES_"
      },
      "source": [
        "# request.py\n",
        "# !python3 request.py /path/2/ur/img\n",
        "import requests\n",
        "from argparse import ArgumentParser\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from io import BufferedReader, BytesIO\n",
        "import base64\n",
        "import threading, queue\n",
        "import multiprocessing as mp\n",
        "import functools\n",
        "\n",
        "from utils import WebcamVideoStream, FPS\n",
        "\n",
        "EVERY_N_FRAME = 4\n",
        "URL = 'http://0.0.0.0:5000/predict'\n",
        "\n",
        "def img_2_io_wrap_img(img):\n",
        "    retval, jpg_enc_img = cv2.imencode(\".jpg\", img)\n",
        "    img_bytes = jpg_enc_img.tobytes()\t\t#将array转化为二进制类型\n",
        "    io_bytes = BytesIO(img_bytes)\t\t#转化为_io.BytesIO类型\n",
        "    # io_bytes.name = \"Cat03.jpg\"\t\t#名称赋值\n",
        "    return BufferedReader(io_bytes)\t\t#转化为_io.BufferedReader类型\n",
        "\n",
        "def single_image_mode(args):\n",
        "    assert os.path.exists(args.image_path) \n",
        "    resp = requests.post(URL, files={\"file\": open(args.image_path,'rb')})  \n",
        "\n",
        "    assert resp.status_code == 200, f\"request failed, status code: {resp.status_code}\"\n",
        "    img = cv2.imread(args.image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    result = resp.json()\n",
        "    cv2.putText(img, f\"V:{result['velance']:.2f}, A:{result['arousal']:.2f}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "    cv2.imshow(img)\n",
        "    cv2.waitkey(0)\n",
        "\n",
        "def real_time_mode(args):\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    count = 0\n",
        "    while(cap.isOpened()):\n",
        "        if count%5 == 0:\n",
        "            count = 0\n",
        "            fps = FPS().start() \n",
        "        elif (count+1)%5 == 0:\n",
        "            fps.stop()\n",
        "            print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
        "            print(\"[INFO] approx. FPS: {:.1f}\".format(fps.fps()))\n",
        "        fps.update()\n",
        "        count += 1\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "        if ret:             \n",
        "            resp = requests.post(URL, files={\"image_io\": img_2_io_wrap_img(frame)})\n",
        "            assert resp.status_code == 200, f\"request failed, status code: {resp.status_code}\"\n",
        "            result = resp.json()\n",
        "            cv2.putText(frame, f\"V:{result['velance']:.2f}, A:{result['arousal']:.2f}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.imshow(\"Video\", frame)\n",
        "\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break     \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    arg_parser = ArgumentParser(\n",
        "    usage='Usage: python ' + __file__ + '[-p <image_path>] [--help]\\nIf image_path is not passed then it will run in real time mode'\n",
        "    )\n",
        "    arg_parser.add_argument('-p', '--image-path', type=str, help=\"path of image to predict if real-time mode is not used\")\n",
        "\n",
        "\n",
        "    args = arg_parser.parse_args()\n",
        "\n",
        "    if args.image_path:\n",
        "        single_image_mode(args)\n",
        "    else:\n",
        "        print(\"\\nreal_time_mode\\n\")\n",
        "        real_time_mode(args)\n",
        "\n",
        "    cv2.destroyAllWindows()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDxhwcDWr93-"
      },
      "source": [
        "# <a name=\"ex\">Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp8A2giwocbr"
      },
      "source": [
        "## Image Data Transfer Speed: Three way to transfer image bytes through network (Encoded at client side then decoded at server side)\n",
        "\n",
        "Different systems interpreting some bytes like \\r \\b in different way, we cant directly passing image bytes through internet\n",
        "\n",
        "* client side:\n",
        "    1. base64 encoded: using 64 characters to encode 00 000000 ~ 00 111111\n",
        "    2. wrap using io\n",
        "    3. direct io\n",
        "* server side\n",
        "    1. dec 2 numpy image\n",
        "    2. dec 2 pil image\n",
        "\n",
        "* transform approach\n",
        "\n",
        "    1. for pil: pil + transforms \n",
        "    2. for numpy:\n",
        "        1. np => pil + transforms (the same output as 1.)\n",
        "        2. np + Albumentation\n",
        "        3. np + cv2 transform + torch tensor (the same output as 3.)\n",
        "\n",
        "* summary\n",
        "        [['approach_io_np_t3', 3.908006429672241],\n",
        "        ['approach_io_np_t2', 3.9259140491485596],\n",
        "        ['approach_io_pil', 4.82680869102478],\n",
        "        ['approach_wrap_io_pil', 4.844152450561523],\n",
        "        ['approach_io_np_t1', 5.604491949081421],\n",
        "        ['approach_wio_np_t3', 11.306720972061157],\n",
        "        ['approach_wio_np_t2', 11.346932411193848],\n",
        "        ['approach_b64_np_t2', 11.60984992980957],\n",
        "        ['approach_b64_np_t3', 11.618371725082397],\n",
        "        ['approach_wio_pil', 12.65789532661438],\n",
        "        ['approach_b64_pil', 13.032054662704468],\n",
        "        ['approach_wio_np_t1', 13.037485122680664],\n",
        "        ['approach_b64_np_t1', 13.37218427658081]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSW9wxzkp84s"
      },
      "source": [
        "## 測試1: 在 1 training epoch 中: 先解壓縮檔案然後load所有圖片會花的時間 vs 每次直接讀取特定 frame "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb9ZpicIq53G",
        "outputId": "7d0d0b62-8e9c-4e93-e1f8-00b067f79585"
      },
      "source": [
        "## read target video target frame\n",
        "# start = time.time()\n",
        "\n",
        "%timeit -n 10 for num in range(1,101):img = read_target_frame(file_path, num)\n",
        "# end = time.time()\n",
        "# print(f'{(end - start):.5f}s per 100 pics by target video and frame')\n",
        "\n",
        "#read by cv2\n",
        "%timeit -n 10 for num in range(1,101):img = Image.fromarray(cv2.imread(f'/content/image/{num}.jpg'))\n",
        "\n",
        "\n",
        "#read by PIL\n",
        "%timeit -n 10  for num in range(1,101):img = Image.open(f'/content/image/{num}.jpg')\n",
        "\n",
        "#y1: 先解壓縮然後load所有圖片會花的時間\n",
        "#y2: 直接讀特定frame花的時間\n",
        "total_pics = 1008463\n",
        "training_epoch = 6\n",
        "x = range(total_pics * training_epoch)\n",
        "y1 = [(nums*(1/1400*180 + 1/100*0.008) if nums<total_pics else total_pics*180/1400 + nums/100*0.008) for nums in x]\n",
        "y2 = [nums/100*4 for nums in x]\n",
        "plt.plot(x, y1)\n",
        "plt.plot(x, y2) del x, y1, y2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 3.98 s per loop\n",
            "10 loops, best of 3: 348 ms per loop\n",
            "10 loops, best of 3: 7.89 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUaw0bIpDqM"
      },
      "source": [
        "## 測試2: custom transform vs torchvision.transforms 花的時間\n",
        "\n",
        "* summary: numpy + custom 最小快兩倍以上(resize)，最多很多"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWajLJ9RtVv2",
        "outputId": "f6758f49-2cc9-4217-da43-f57dc6635a7c"
      },
      "source": [
        "# custom is much faster than pytorch transform\n",
        "a_np = np.zeros((640,440,3))\n",
        "a_pil = Image.fromarray(np.uint8(np.zeros((640,440,3))))\n",
        "totensor = transforms.ToTensor()\n",
        "totensor_a = ToTensorV2()\n",
        "resizer = transforms.Resize(256)\n",
        "resizer_a = A.Resize(256, 256)\n",
        "randomcroper = transforms.RandomCrop(224)\n",
        "randomcroper_a = A.RandomCrop(224, 224)\n",
        "def custom_resize(image):\n",
        "  output_size = 256\n",
        "  h, w = image.shape[:2]\n",
        "  if h > w:\n",
        "    new_h, new_w = int(h * output_size / w), output_size\n",
        "  else:\n",
        "    new_h, new_w = output_size, int(w * output_size / h)\n",
        "  return cv2.resize(image, dsize=(new_w, new_h))\n",
        "\n",
        "def custom_randomcrop(image):\n",
        "  h, w = image.shape[:2]\n",
        "  new_h, new_w = 256, 256\n",
        "  top = np.random.randint(0, h - new_h)\n",
        "  left = np.random.randint(0, w - new_w)\n",
        "\n",
        "  return image[top: top + new_h,\n",
        "                left: left + new_w]\n",
        "print('\\nToTensor')\n",
        "print(\"- transform (for pil)\")\n",
        "%timeit -n 10 totensor(a_pil)\n",
        "print(\"- transform (for np)\")\n",
        "%timeit -n 10 totensor(a_np) \n",
        "print(\"- A.ToTensorV2 (for np)\")\n",
        "%timeit -n 10 totensor_a(image=a_np) \n",
        "print(\"- custom (for np)\")\n",
        "%timeit -n 10 torch.from_numpy(a_np)\n",
        "\n",
        "print('\\nresizer')\n",
        "print(\"- transform (for pil)\")\n",
        "%timeit -n 10 resizer(a_pil)\n",
        "print(\"- A.Resize (for np)\")\n",
        "%timeit -n 10 resizer_a(image=a_np)\n",
        "print(\"- custom (for np)\")\n",
        "%timeit -n 10 custom_resize(a_np)\n",
        "\n",
        "print('\\nrandomCrop')\n",
        "print(\"- transform (for pil)\")\n",
        "%timeit -n 10 randomcroper(a_pil)\n",
        "print(\"- A.randomcrop (for np)\")\n",
        "%timeit -n 10 randomcroper_a(image=a_np)\n",
        "print(\"- custom (for np)\")\n",
        "%timeit -n 10 custom_randomcrop(a_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ToTensor\n",
            "- transform (for pil)\n",
            "10 loops, best of 5: 2.87 ms per loop\n",
            "- transform (for np)\n",
            "10 loops, best of 5: 1.9 ms per loop\n",
            "- A.ToTensorV2 (for np)\n",
            "10 loops, best of 5: 7.67 µs per loop\n",
            "- custom (for np)\n",
            "10 loops, best of 5: 794 ns per loop\n",
            "\n",
            "resizer\n",
            "- transform (for pil)\n",
            "10 loops, best of 5: 2.8 ms per loop\n",
            "- A.Resize (for np)\n",
            "10 loops, best of 5: 1.19 ms per loop\n",
            "- custom (for np)\n",
            "10 loops, best of 5: 1.63 ms per loop\n",
            "\n",
            "randomCrop\n",
            "- transform (for pil)\n",
            "The slowest run took 6.38 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10 loops, best of 5: 34.3 µs per loop\n",
            "- A.randomcrop (for np)\n",
            "10 loops, best of 5: 8.74 µs per loop\n",
            "- custom (for np)\n",
            "10 loops, best of 5: 7.23 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw7x6_Y_pTZA"
      },
      "source": [
        "## 測試3. approach to transfer image through network and then make model input well-prepared\n",
        "\n",
        "* summary\n",
        "        [['approach_io_np_t3', 3.908006429672241],\n",
        "        ['approach_io_np_t2', 3.9259140491485596],\n",
        "        ['approach_io_pil', 4.82680869102478],\n",
        "        ['approach_wrap_io_pil', 4.844152450561523],\n",
        "        ['approach_io_np_t1', 5.604491949081421],\n",
        "        ['approach_wio_np_t3', 11.306720972061157],\n",
        "        ['approach_wio_np_t2', 11.346932411193848],\n",
        "        ['approach_b64_np_t2', 11.60984992980957],\n",
        "        ['approach_b64_np_t3', 11.618371725082397],\n",
        "        ['approach_wio_pil', 12.65789532661438],\n",
        "        ['approach_b64_pil', 13.032054662704468],\n",
        "        ['approach_wio_np_t1', 13.037485122680664],\n",
        "        ['approach_b64_np_t1', 13.37218427658081]]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Js1xp8p0HH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4416d5-4ba4-449e-d280-3fac6ddee9ca"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps albumentations\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BufferedReader, BytesIO\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# - - - - - - - - - - -\n",
        "# approach 1: enc dec through base64\n",
        "# - - - - - - - - - - -\n",
        "def base64_enc(img):\n",
        "    retval, jpg_enc_img = cv2.imencode(\".jpg\", img)\n",
        "    return base64.b64encode(jpg_enc_img)\n",
        "\n",
        "def dec_2_np_img(b64enc_img_bytes):\n",
        "    img_bytes = base64.b64decode(b64enc_img_bytes)\n",
        "    jpg_enc_img = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "    return cv2.imdecode(jpg_enc_img, cv2.IMREAD_COLOR)\n",
        "\n",
        "def dec_2_pil_img(b64enc_img_bytes):\n",
        "    img_bytes = base64.b64decode(b64enc_img_bytes)\n",
        "    return Image.open(io.BytesIO(img_bytes))\n",
        "\n",
        "# - - - - - - - - - - -\n",
        "# approach 2: pass open(, \"rb\") (io.BufferReader) directly\n",
        "# - - - - - - - - - - -\n",
        "\n",
        "# image_io = open(\"/content/Cat03.jpg\", \"rb\")\n",
        "\n",
        "# - - - - - - - - - - -\n",
        "# approach 3: wrap loaded image using io (模仿2)\n",
        "# - - - - - - - - - - -\n",
        "def img_2_io_wrap_img(img):\n",
        "    ret, jpg_enc_img = cv2.imencode('.jpg', img)\n",
        "    img_bytes = jpg_enc_img.tobytes()\t\t#将array转化为二进制类型\n",
        "    io_bytes = BytesIO(img_bytes)\t\t#转化为_io.BytesIO类型\n",
        "    # io_bytes.name = \"Cat03.jpg\"\t\t#名称赋值    \n",
        "    return BufferedReader(io_bytes)\t\t#转化为_io.BufferedReader类型\n",
        "\n",
        "def io_wrap_img_2_np_img(image_io):\n",
        "    img_bytes = image_io.read()\n",
        "    image_buffer = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "    return cv2.imdecode(image_buffer, cv2.IMREAD_COLOR)\n",
        "\n",
        "def io_wrap_img_2_pil_img(image_io):\n",
        "    img_bytes = image_io.read()    \n",
        "    return Image.open(io.BytesIO(img_bytes))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# transformation\n",
        "def transform_pil_img(pil_img):    \n",
        "    comp_transfroms = transforms.Compose([\n",
        "                                      transforms.Resize(224),\n",
        "                                      transforms.ToTensor()\n",
        "                                    ])    \n",
        "    return comp_transfroms(pil_img).unsqueeze(0)\n",
        "\n",
        "def transform_1_np_img(np_img):\n",
        "    np_img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n",
        "    pil_img = Image.fromarray(np_img)\n",
        "    comp_transfroms = transforms.Compose([\n",
        "                                      transforms.Resize(224),\n",
        "                                      transforms.ToTensor()\n",
        "                                    ])    \n",
        "    return comp_transfroms(pil_img).unsqueeze(0)\n",
        "\n",
        "def transform_2_np_img(np_img):\n",
        "    np_img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n",
        "    transform = A.Compose([      \n",
        "                        A.Resize(224, 224),  # 變形                                                                             \n",
        "                        ToTensorV2()\n",
        "                ])\n",
        "    return transform(image=np_img)['image'].div(255.0).unsqueeze(0)\n",
        "\n",
        "def transform_3_np_img(np_img):\n",
        "    np_img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n",
        "    np_img = cv2.resize(np_img, (224, 224))\n",
        "    tensor = torch.tensor(np_img, dtype=torch.float)    \n",
        "    return tensor.permute(2, 0, 1).div(255.0).unsqueeze(0)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations\n",
            "  Downloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.3\n",
            "--2021-09-05 12:03:43--  https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 103.102.166.240, 2001:df2:e500:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|103.102.166.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279603 (273K) [image/jpeg]\n",
            "Saving to: ‘Cat03.jpg.2’\n",
            "\n",
            "Cat03.jpg.2         100%[===================>] 273.05K  1.32MB/s    in 0.2s    \n",
            "\n",
            "2021-09-05 12:03:44 (1.32 MB/s) - ‘Cat03.jpg.2’ saved [279603/279603]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWYCo2BIhVWV",
        "outputId": "b1101f44-74b6-420f-a840-ffaf948e2ee4"
      },
      "source": [
        "def evaluate(func):\n",
        "    start = time.time()\n",
        "    for _ in range(100):\n",
        "        model_input = func()\n",
        "        if _ == 0:\n",
        "            print(model_input)\n",
        "    end = time.time()\n",
        "    print(end - start)\n",
        "    return end - start\n",
        "\n",
        "# pil-1-b64\n",
        "def approach_b64_pil():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    enc_img = base64_enc(img)\n",
        "    pil_img = dec_2_pil_img(enc_img)    \n",
        "    return transform_pil_img(pil_img)\n",
        "\n",
        "# pil-2-wrap-io\n",
        "def approach_wio_pil():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    image_io = img_2_io_wrap_img(img)\n",
        "    pil_img = io_wrap_img_2_pil_img(image_io)    \n",
        "    return transform_pil_img(pil_img)\n",
        "\n",
        "# pil-3-io\n",
        "def approach_io_pil():\n",
        "    image_io = open(\"/content/Cat03.jpg\", \"rb\")\n",
        "    pil_img = io_wrap_img_2_pil_img(image_io)    \n",
        "    return transform_pil_img(pil_img)\n",
        "\n",
        "\n",
        "# np-b64-t1\n",
        "def approach_b64_np_t1():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    enc_img = base64_enc(img)\n",
        "    np_img = dec_2_np_img(enc_img)    \n",
        "    return transform_1_np_img(np_img)\n",
        " \n",
        "# np-b64-t2\n",
        "def approach_b64_np_t2():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    enc_img = base64_enc(img)\n",
        "    np_img = dec_2_np_img(enc_img)    \n",
        "    return transform_2_np_img(np_img)\n",
        "\n",
        "# np-b64-t3\n",
        "def approach_b64_np_t3():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    enc_img = base64_enc(img)\n",
        "    np_img = dec_2_np_img(enc_img)    \n",
        "    return transform_3_np_img(np_img)\n",
        "\n",
        "# np-wio-t1\n",
        "def approach_wio_np_t1():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    image_io = img_2_io_wrap_img(img)\n",
        "    np_img = io_wrap_img_2_np_img(image_io)\n",
        "    return transform_1_np_img(np_img)\n",
        "\n",
        "# np-wio-t2\n",
        "def approach_wio_np_t2():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    image_io = img_2_io_wrap_img(img)\n",
        "    np_img = io_wrap_img_2_np_img(image_io)\n",
        "    return transform_2_np_img(np_img)\n",
        "\n",
        "# np-wio-t3\n",
        "def approach_wio_np_t3():\n",
        "    img = cv2.imread(\"/content/Cat03.jpg\")\n",
        "    image_io = img_2_io_wrap_img(img)\n",
        "    np_img = io_wrap_img_2_np_img(image_io)\n",
        "    return transform_3_np_img(np_img)\n",
        "\n",
        "# np-io-t1\n",
        "def approach_io_np_t1():  \n",
        "    image_io = open(\"/content/Cat03.jpg\", \"rb\")\n",
        "    np_img = io_wrap_img_2_np_img(image_io)\n",
        "    return transform_1_np_img(np_img)\n",
        "\n",
        "# np-io-t2\n",
        "def approach_io_np_t2():  \n",
        "    image_io = open(\"/content/Cat03.jpg\", \"rb\")\n",
        "    np_img = io_wrap_img_2_np_img(image_io)\n",
        "    return transform_2_np_img(np_img)\n",
        "\n",
        "# np-io-t3\n",
        "def approach_io_np_t3():  \n",
        "    image_io = open(\"/content/Cat03.jpg\", \"rb\")\n",
        "    np_img = io_wrap_img_2_np_img(image_io)\n",
        "    return transform_3_np_img(np_img)\n",
        "\n",
        "record = []\n",
        "g = globals().copy()\n",
        "for k, v in g.items():\n",
        "    if \"approach\" in k:\n",
        "        print(k)\n",
        "        time_cost = evaluate(v)\n",
        "        record.append([k, time_cost])\n",
        "print(record)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "approach_b64_pil\n",
            "tensor([[[[0.8549, 0.8667, 0.8745,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8510, 0.8667, 0.8784,  ..., 0.9137, 0.9176, 0.9216],\n",
            "          [0.8510, 0.8627, 0.8745,  ..., 0.9216, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.7373, 0.7451, 0.7333,  ..., 0.8588, 0.8627, 0.8549],\n",
            "          [0.7373, 0.7451, 0.7373,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7333, 0.7333, 0.7333,  ..., 0.8745, 0.8784, 0.8824]],\n",
            "\n",
            "         [[0.8196, 0.8314, 0.8431,  ..., 0.8784, 0.8745, 0.8706],\n",
            "          [0.8157, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8353, 0.8510,  ..., 0.8745, 0.8667, 0.8549],\n",
            "          ...,\n",
            "          [0.6157, 0.6275, 0.6118,  ..., 0.8078, 0.8118, 0.8118],\n",
            "          [0.6196, 0.6275, 0.6196,  ..., 0.8235, 0.8235, 0.8196],\n",
            "          [0.6078, 0.6157, 0.6157,  ..., 0.8275, 0.8314, 0.8353]],\n",
            "\n",
            "         [[0.6980, 0.7098, 0.7412,  ..., 0.8275, 0.8235, 0.8235],\n",
            "          [0.6863, 0.7098, 0.7412,  ..., 0.8314, 0.8275, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8353, 0.8275, 0.8196],\n",
            "          ...,\n",
            "          [0.4706, 0.4824, 0.4706,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          [0.4824, 0.4941, 0.4824,  ..., 0.7608, 0.7608, 0.7569],\n",
            "          [0.4745, 0.4824, 0.4824,  ..., 0.7647, 0.7686, 0.7725]]]])\n",
            "13.032054662704468\n",
            "approach_wrap_io_pil\n",
            "tensor([[[[0.8549, 0.8667, 0.8745,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8510, 0.8667, 0.8784,  ..., 0.9137, 0.9176, 0.9176],\n",
            "          [0.8510, 0.8627, 0.8745,  ..., 0.9255, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7333,  ..., 0.8588, 0.8588, 0.8549],\n",
            "          [0.7373, 0.7451, 0.7373,  ..., 0.8706, 0.8706, 0.8706],\n",
            "          [0.7333, 0.7333, 0.7333,  ..., 0.8745, 0.8784, 0.8824]],\n",
            "\n",
            "         [[0.8196, 0.8314, 0.8431,  ..., 0.8784, 0.8745, 0.8706],\n",
            "          [0.8157, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8706],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8667, 0.8549],\n",
            "          ...,\n",
            "          [0.6157, 0.6235, 0.6118,  ..., 0.8078, 0.8118, 0.8118],\n",
            "          [0.6196, 0.6275, 0.6196,  ..., 0.8235, 0.8235, 0.8196],\n",
            "          [0.6078, 0.6157, 0.6157,  ..., 0.8275, 0.8314, 0.8353]],\n",
            "\n",
            "         [[0.6980, 0.7098, 0.7412,  ..., 0.8275, 0.8235, 0.8235],\n",
            "          [0.6863, 0.7098, 0.7412,  ..., 0.8314, 0.8275, 0.8235],\n",
            "          [0.6824, 0.7098, 0.7333,  ..., 0.8353, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4824, 0.4706,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          [0.4824, 0.4941, 0.4824,  ..., 0.7608, 0.7608, 0.7569],\n",
            "          [0.4745, 0.4824, 0.4824,  ..., 0.7647, 0.7686, 0.7725]]]])\n",
            "4.844152450561523\n",
            "approach_b64_np_t1\n",
            "tensor([[[[0.8549, 0.8667, 0.8745,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8510, 0.8667, 0.8784,  ..., 0.9137, 0.9176, 0.9216],\n",
            "          [0.8510, 0.8627, 0.8745,  ..., 0.9216, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7333,  ..., 0.8588, 0.8588, 0.8549],\n",
            "          [0.7373, 0.7451, 0.7373,  ..., 0.8706, 0.8706, 0.8667],\n",
            "          [0.7333, 0.7333, 0.7333,  ..., 0.8745, 0.8784, 0.8824]],\n",
            "\n",
            "         [[0.8196, 0.8314, 0.8471,  ..., 0.8784, 0.8706, 0.8706],\n",
            "          [0.8157, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8353, 0.8510,  ..., 0.8745, 0.8667, 0.8549],\n",
            "          ...,\n",
            "          [0.6157, 0.6275, 0.6118,  ..., 0.8078, 0.8118, 0.8118],\n",
            "          [0.6196, 0.6275, 0.6196,  ..., 0.8235, 0.8235, 0.8196],\n",
            "          [0.6078, 0.6157, 0.6157,  ..., 0.8275, 0.8314, 0.8353]],\n",
            "\n",
            "         [[0.6980, 0.7098, 0.7412,  ..., 0.8275, 0.8235, 0.8235],\n",
            "          [0.6863, 0.7098, 0.7412,  ..., 0.8314, 0.8275, 0.8235],\n",
            "          [0.6824, 0.7098, 0.7333,  ..., 0.8353, 0.8275, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4824, 0.4706,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          [0.4824, 0.4941, 0.4824,  ..., 0.7608, 0.7608, 0.7569],\n",
            "          [0.4745, 0.4824, 0.4824,  ..., 0.7647, 0.7686, 0.7725]]]])\n",
            "13.37218427658081\n",
            "approach_b64_np_t2\n",
            "tensor([[[[0.8471, 0.8667, 0.8745,  ..., 0.9059, 0.9137, 0.9098],\n",
            "          [0.8431, 0.8667, 0.8706,  ..., 0.9137, 0.9255, 0.9216],\n",
            "          [0.8549, 0.8667, 0.8667,  ..., 0.9176, 0.9176, 0.9216],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7373,  ..., 0.8588, 0.8588, 0.8588],\n",
            "          [0.7412, 0.7490, 0.7333,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7294, 0.7294, 0.7294,  ..., 0.8745, 0.8784, 0.8863]],\n",
            "\n",
            "         [[0.8118, 0.8314, 0.8471,  ..., 0.8745, 0.8706, 0.8706],\n",
            "          [0.8078, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8706, 0.8549],\n",
            "          ...,\n",
            "          [0.6196, 0.6275, 0.6157,  ..., 0.8118, 0.8118, 0.8118],\n",
            "          [0.6235, 0.6314, 0.6157,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6000, 0.6118, 0.6118,  ..., 0.8275, 0.8314, 0.8392]],\n",
            "\n",
            "         [[0.6902, 0.7098, 0.7412,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7451,  ..., 0.8392, 0.8314, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8392, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4784, 0.4745,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.4824, 0.4980, 0.4902,  ..., 0.7608, 0.7608, 0.7608],\n",
            "          [0.4706, 0.4784, 0.4784,  ..., 0.7647, 0.7686, 0.7765]]]])\n",
            "11.60984992980957\n",
            "approach_b64_np_t3\n",
            "tensor([[[[0.8471, 0.8667, 0.8745,  ..., 0.9059, 0.9137, 0.9098],\n",
            "          [0.8431, 0.8667, 0.8706,  ..., 0.9137, 0.9255, 0.9216],\n",
            "          [0.8549, 0.8667, 0.8667,  ..., 0.9176, 0.9176, 0.9216],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7373,  ..., 0.8588, 0.8588, 0.8588],\n",
            "          [0.7412, 0.7490, 0.7333,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7294, 0.7294, 0.7294,  ..., 0.8745, 0.8784, 0.8863]],\n",
            "\n",
            "         [[0.8118, 0.8314, 0.8471,  ..., 0.8745, 0.8706, 0.8706],\n",
            "          [0.8078, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8706, 0.8549],\n",
            "          ...,\n",
            "          [0.6196, 0.6275, 0.6157,  ..., 0.8118, 0.8118, 0.8118],\n",
            "          [0.6235, 0.6314, 0.6157,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6000, 0.6118, 0.6118,  ..., 0.8275, 0.8314, 0.8392]],\n",
            "\n",
            "         [[0.6902, 0.7098, 0.7412,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7451,  ..., 0.8392, 0.8314, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8392, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4784, 0.4745,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.4824, 0.4980, 0.4902,  ..., 0.7608, 0.7608, 0.7608],\n",
            "          [0.4706, 0.4784, 0.4784,  ..., 0.7647, 0.7686, 0.7765]]]])\n",
            "11.618371725082397\n",
            "approach_wio_np_t1\n",
            "tensor([[[[0.8549, 0.8667, 0.8745,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8510, 0.8667, 0.8784,  ..., 0.9137, 0.9176, 0.9216],\n",
            "          [0.8510, 0.8627, 0.8745,  ..., 0.9216, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7333,  ..., 0.8588, 0.8588, 0.8549],\n",
            "          [0.7373, 0.7451, 0.7373,  ..., 0.8706, 0.8706, 0.8667],\n",
            "          [0.7333, 0.7333, 0.7333,  ..., 0.8745, 0.8784, 0.8824]],\n",
            "\n",
            "         [[0.8196, 0.8314, 0.8471,  ..., 0.8784, 0.8706, 0.8706],\n",
            "          [0.8157, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8353, 0.8510,  ..., 0.8745, 0.8667, 0.8549],\n",
            "          ...,\n",
            "          [0.6157, 0.6275, 0.6118,  ..., 0.8078, 0.8118, 0.8118],\n",
            "          [0.6196, 0.6275, 0.6196,  ..., 0.8235, 0.8235, 0.8196],\n",
            "          [0.6078, 0.6157, 0.6157,  ..., 0.8275, 0.8314, 0.8353]],\n",
            "\n",
            "         [[0.6980, 0.7098, 0.7412,  ..., 0.8275, 0.8235, 0.8235],\n",
            "          [0.6863, 0.7098, 0.7412,  ..., 0.8314, 0.8275, 0.8235],\n",
            "          [0.6824, 0.7098, 0.7333,  ..., 0.8353, 0.8275, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4824, 0.4706,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          [0.4824, 0.4941, 0.4824,  ..., 0.7608, 0.7608, 0.7569],\n",
            "          [0.4745, 0.4824, 0.4824,  ..., 0.7647, 0.7686, 0.7725]]]])\n",
            "13.037485122680664\n",
            "approach_wio_np_t2\n",
            "tensor([[[[0.8471, 0.8667, 0.8745,  ..., 0.9059, 0.9137, 0.9098],\n",
            "          [0.8431, 0.8667, 0.8706,  ..., 0.9137, 0.9255, 0.9216],\n",
            "          [0.8549, 0.8667, 0.8667,  ..., 0.9176, 0.9176, 0.9216],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7373,  ..., 0.8588, 0.8588, 0.8588],\n",
            "          [0.7412, 0.7490, 0.7333,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7294, 0.7294, 0.7294,  ..., 0.8745, 0.8784, 0.8863]],\n",
            "\n",
            "         [[0.8118, 0.8314, 0.8471,  ..., 0.8745, 0.8706, 0.8706],\n",
            "          [0.8078, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8706, 0.8549],\n",
            "          ...,\n",
            "          [0.6196, 0.6275, 0.6157,  ..., 0.8118, 0.8118, 0.8118],\n",
            "          [0.6235, 0.6314, 0.6157,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6000, 0.6118, 0.6118,  ..., 0.8275, 0.8314, 0.8392]],\n",
            "\n",
            "         [[0.6902, 0.7098, 0.7412,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7451,  ..., 0.8392, 0.8314, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8392, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4784, 0.4745,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.4824, 0.4980, 0.4902,  ..., 0.7608, 0.7608, 0.7608],\n",
            "          [0.4706, 0.4784, 0.4784,  ..., 0.7647, 0.7686, 0.7765]]]])\n",
            "11.346932411193848\n",
            "approach_wio_np_t3\n",
            "tensor([[[[0.8471, 0.8667, 0.8745,  ..., 0.9059, 0.9137, 0.9098],\n",
            "          [0.8431, 0.8667, 0.8706,  ..., 0.9137, 0.9255, 0.9216],\n",
            "          [0.8549, 0.8667, 0.8667,  ..., 0.9176, 0.9176, 0.9216],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7373,  ..., 0.8588, 0.8588, 0.8588],\n",
            "          [0.7412, 0.7490, 0.7333,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7294, 0.7294, 0.7294,  ..., 0.8745, 0.8784, 0.8863]],\n",
            "\n",
            "         [[0.8118, 0.8314, 0.8471,  ..., 0.8745, 0.8706, 0.8706],\n",
            "          [0.8078, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8706, 0.8549],\n",
            "          ...,\n",
            "          [0.6196, 0.6275, 0.6157,  ..., 0.8118, 0.8118, 0.8118],\n",
            "          [0.6235, 0.6314, 0.6157,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6000, 0.6118, 0.6118,  ..., 0.8275, 0.8314, 0.8392]],\n",
            "\n",
            "         [[0.6902, 0.7098, 0.7412,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7451,  ..., 0.8392, 0.8314, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8392, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4784, 0.4745,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.4824, 0.4980, 0.4902,  ..., 0.7608, 0.7608, 0.7608],\n",
            "          [0.4706, 0.4784, 0.4784,  ..., 0.7647, 0.7686, 0.7765]]]])\n",
            "11.306720972061157\n",
            "approach_io_np_t1\n",
            "tensor([[[[0.8549, 0.8667, 0.8745,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8510, 0.8667, 0.8784,  ..., 0.9137, 0.9176, 0.9176],\n",
            "          [0.8510, 0.8627, 0.8745,  ..., 0.9255, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7333,  ..., 0.8588, 0.8588, 0.8549],\n",
            "          [0.7373, 0.7451, 0.7373,  ..., 0.8706, 0.8706, 0.8706],\n",
            "          [0.7333, 0.7333, 0.7333,  ..., 0.8745, 0.8784, 0.8824]],\n",
            "\n",
            "         [[0.8196, 0.8314, 0.8431,  ..., 0.8784, 0.8745, 0.8706],\n",
            "          [0.8157, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8706],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8667, 0.8549],\n",
            "          ...,\n",
            "          [0.6157, 0.6235, 0.6118,  ..., 0.8078, 0.8118, 0.8118],\n",
            "          [0.6196, 0.6275, 0.6196,  ..., 0.8235, 0.8235, 0.8196],\n",
            "          [0.6078, 0.6157, 0.6157,  ..., 0.8275, 0.8314, 0.8353]],\n",
            "\n",
            "         [[0.6980, 0.7098, 0.7412,  ..., 0.8275, 0.8235, 0.8235],\n",
            "          [0.6863, 0.7098, 0.7412,  ..., 0.8314, 0.8275, 0.8235],\n",
            "          [0.6824, 0.7098, 0.7333,  ..., 0.8353, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4824, 0.4706,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          [0.4824, 0.4941, 0.4824,  ..., 0.7608, 0.7608, 0.7569],\n",
            "          [0.4745, 0.4824, 0.4824,  ..., 0.7647, 0.7686, 0.7725]]]])\n",
            "5.604491949081421\n",
            "approach_io_np_t2\n",
            "tensor([[[[0.8471, 0.8667, 0.8706,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8431, 0.8667, 0.8745,  ..., 0.9098, 0.9176, 0.9176],\n",
            "          [0.8549, 0.8667, 0.8706,  ..., 0.9255, 0.9176, 0.9216],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7373,  ..., 0.8588, 0.8588, 0.8588],\n",
            "          [0.7412, 0.7490, 0.7333,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7294, 0.7294, 0.7294,  ..., 0.8745, 0.8784, 0.8863]],\n",
            "\n",
            "         [[0.8118, 0.8314, 0.8431,  ..., 0.8745, 0.8706, 0.8706],\n",
            "          [0.8078, 0.8314, 0.8471,  ..., 0.8863, 0.8784, 0.8706],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8706, 0.8549],\n",
            "          ...,\n",
            "          [0.6196, 0.6275, 0.6157,  ..., 0.8118, 0.8118, 0.8118],\n",
            "          [0.6235, 0.6314, 0.6157,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6000, 0.6118, 0.6118,  ..., 0.8275, 0.8314, 0.8392]],\n",
            "\n",
            "         [[0.6902, 0.7098, 0.7412,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6824, 0.7098, 0.7451,  ..., 0.8392, 0.8314, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8392, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4784, 0.4745,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.4824, 0.4980, 0.4902,  ..., 0.7608, 0.7608, 0.7608],\n",
            "          [0.4706, 0.4784, 0.4784,  ..., 0.7647, 0.7686, 0.7765]]]])\n",
            "3.9259140491485596\n",
            "approach_io_np_t3\n",
            "tensor([[[[0.8471, 0.8667, 0.8706,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8431, 0.8667, 0.8745,  ..., 0.9098, 0.9176, 0.9176],\n",
            "          [0.8549, 0.8667, 0.8706,  ..., 0.9255, 0.9176, 0.9216],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7373,  ..., 0.8588, 0.8588, 0.8588],\n",
            "          [0.7412, 0.7490, 0.7333,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7294, 0.7294, 0.7294,  ..., 0.8745, 0.8784, 0.8863]],\n",
            "\n",
            "         [[0.8118, 0.8314, 0.8431,  ..., 0.8745, 0.8706, 0.8706],\n",
            "          [0.8078, 0.8314, 0.8471,  ..., 0.8863, 0.8784, 0.8706],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8706, 0.8549],\n",
            "          ...,\n",
            "          [0.6196, 0.6275, 0.6157,  ..., 0.8118, 0.8118, 0.8118],\n",
            "          [0.6235, 0.6314, 0.6157,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6000, 0.6118, 0.6118,  ..., 0.8275, 0.8314, 0.8392]],\n",
            "\n",
            "         [[0.6902, 0.7098, 0.7412,  ..., 0.8235, 0.8235, 0.8235],\n",
            "          [0.6824, 0.7098, 0.7451,  ..., 0.8392, 0.8314, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8392, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4784, 0.4745,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.4824, 0.4980, 0.4902,  ..., 0.7608, 0.7608, 0.7608],\n",
            "          [0.4706, 0.4784, 0.4784,  ..., 0.7647, 0.7686, 0.7765]]]])\n",
            "3.908006429672241\n",
            "approach_wio_pil\n",
            "tensor([[[[0.8549, 0.8667, 0.8745,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8510, 0.8667, 0.8784,  ..., 0.9137, 0.9176, 0.9216],\n",
            "          [0.8510, 0.8627, 0.8745,  ..., 0.9216, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.7373, 0.7451, 0.7333,  ..., 0.8588, 0.8627, 0.8549],\n",
            "          [0.7373, 0.7451, 0.7373,  ..., 0.8706, 0.8745, 0.8706],\n",
            "          [0.7333, 0.7333, 0.7333,  ..., 0.8745, 0.8784, 0.8824]],\n",
            "\n",
            "         [[0.8196, 0.8314, 0.8431,  ..., 0.8784, 0.8745, 0.8706],\n",
            "          [0.8157, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8667],\n",
            "          [0.8235, 0.8353, 0.8510,  ..., 0.8745, 0.8667, 0.8549],\n",
            "          ...,\n",
            "          [0.6157, 0.6275, 0.6118,  ..., 0.8078, 0.8118, 0.8118],\n",
            "          [0.6196, 0.6275, 0.6196,  ..., 0.8235, 0.8235, 0.8196],\n",
            "          [0.6078, 0.6157, 0.6157,  ..., 0.8275, 0.8314, 0.8353]],\n",
            "\n",
            "         [[0.6980, 0.7098, 0.7412,  ..., 0.8275, 0.8235, 0.8235],\n",
            "          [0.6863, 0.7098, 0.7412,  ..., 0.8314, 0.8275, 0.8235],\n",
            "          [0.6784, 0.7098, 0.7333,  ..., 0.8353, 0.8275, 0.8196],\n",
            "          ...,\n",
            "          [0.4706, 0.4824, 0.4706,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          [0.4824, 0.4941, 0.4824,  ..., 0.7608, 0.7608, 0.7569],\n",
            "          [0.4745, 0.4824, 0.4824,  ..., 0.7647, 0.7686, 0.7725]]]])\n",
            "12.65789532661438\n",
            "approach_io_pil\n",
            "tensor([[[[0.8549, 0.8667, 0.8745,  ..., 0.9059, 0.9098, 0.9098],\n",
            "          [0.8510, 0.8667, 0.8784,  ..., 0.9137, 0.9176, 0.9176],\n",
            "          [0.8510, 0.8627, 0.8745,  ..., 0.9255, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.7373, 0.7490, 0.7333,  ..., 0.8588, 0.8588, 0.8549],\n",
            "          [0.7373, 0.7451, 0.7373,  ..., 0.8706, 0.8706, 0.8706],\n",
            "          [0.7333, 0.7333, 0.7333,  ..., 0.8745, 0.8784, 0.8824]],\n",
            "\n",
            "         [[0.8196, 0.8314, 0.8431,  ..., 0.8784, 0.8745, 0.8706],\n",
            "          [0.8157, 0.8314, 0.8471,  ..., 0.8824, 0.8784, 0.8706],\n",
            "          [0.8235, 0.8314, 0.8510,  ..., 0.8745, 0.8667, 0.8549],\n",
            "          ...,\n",
            "          [0.6157, 0.6235, 0.6118,  ..., 0.8078, 0.8118, 0.8118],\n",
            "          [0.6196, 0.6275, 0.6196,  ..., 0.8235, 0.8235, 0.8196],\n",
            "          [0.6078, 0.6157, 0.6157,  ..., 0.8275, 0.8314, 0.8353]],\n",
            "\n",
            "         [[0.6980, 0.7098, 0.7412,  ..., 0.8275, 0.8235, 0.8235],\n",
            "          [0.6863, 0.7098, 0.7412,  ..., 0.8314, 0.8275, 0.8235],\n",
            "          [0.6824, 0.7098, 0.7333,  ..., 0.8353, 0.8235, 0.8157],\n",
            "          ...,\n",
            "          [0.4706, 0.4824, 0.4706,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          [0.4824, 0.4941, 0.4824,  ..., 0.7608, 0.7608, 0.7569],\n",
            "          [0.4745, 0.4824, 0.4824,  ..., 0.7647, 0.7686, 0.7725]]]])\n",
            "4.82680869102478\n",
            "[['approach_b64_pil', 13.032054662704468], ['approach_wrap_io_pil', 4.844152450561523], ['approach_b64_np_t1', 13.37218427658081], ['approach_b64_np_t2', 11.60984992980957], ['approach_b64_np_t3', 11.618371725082397], ['approach_wio_np_t1', 13.037485122680664], ['approach_wio_np_t2', 11.346932411193848], ['approach_wio_np_t3', 11.306720972061157], ['approach_io_np_t1', 5.604491949081421], ['approach_io_np_t2', 3.9259140491485596], ['approach_io_np_t3', 3.908006429672241], ['approach_wio_pil', 12.65789532661438], ['approach_io_pil', 4.82680869102478]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiJiT0ff35sT",
        "outputId": "84537065-9ecb-4fec-c515-43a7b5384c48"
      },
      "source": [
        "sorted(record, key=lambda x:x[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['approach_io_np_t3', 3.908006429672241],\n",
              " ['approach_io_np_t2', 3.9259140491485596],\n",
              " ['approach_io_pil', 4.82680869102478],\n",
              " ['approach_wrap_io_pil', 4.844152450561523],\n",
              " ['approach_io_np_t1', 5.604491949081421],\n",
              " ['approach_wio_np_t3', 11.306720972061157],\n",
              " ['approach_wio_np_t2', 11.346932411193848],\n",
              " ['approach_b64_np_t2', 11.60984992980957],\n",
              " ['approach_b64_np_t3', 11.618371725082397],\n",
              " ['approach_wio_pil', 12.65789532661438],\n",
              " ['approach_b64_pil', 13.032054662704468],\n",
              " ['approach_wio_np_t1', 13.037485122680664],\n",
              " ['approach_b64_np_t1', 13.37218427658081]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vZDNSvj6yzCf",
        "outputId": "85c40e44-decf-4e33-ffaa-cfe982a46515"
      },
      "source": [
        "a =          [['approach_io_np_t3', 3.908006429672241],\n",
        "          ['approach_io_np_t2', 3.9259140491485596],\n",
        "          ['approach_io_pil', 4.82680869102478],\n",
        "          ['approach_io_np_t1', 5.604491949081421],\n",
        "          ['approach_wio_np_t3', 11.306720972061157],\n",
        "          ['approach_wio_np_t2', 11.346932411193848],\n",
        "          ['approach_b64_np_t2', 11.60984992980957],\n",
        "          ['approach_b64_np_t3', 11.618371725082397],\n",
        "          ['approach_wio_pil', 12.65789532661438],\n",
        "          ['approach_b64_pil', 13.032054662704468],\n",
        "          ['approach_wio_np_t1', 13.037485122680664],\n",
        "          ['approach_b64_np_t1', 13.37218427658081]]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(a, columns=[\"approach\", \"time_spent(second)\"])\n",
        "df[\"approach\"] = df[\"approach\"].apply(lambda x: x.replace(\"approach_\", \"\"))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "ax = sns.barplot(x=\"approach\", y=\"time_spent(second)\", data=df, palette=\"rocket\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "ax.set_title(\"Time spent (second) on different approach\")\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}